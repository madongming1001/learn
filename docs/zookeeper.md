# CAP定理和BASE理论

## 分布式事务的两个理论模型

### CAP定理

CAP定理，又叫布鲁尔定理。简单来说它是指在分布式系统中不可能同时满足一致性(C:Consistency)、可用性(A:Availability)
、分区容错性(P:partition Tolerance) 这三个基本需求，最多同时满足两个。

- C：数据在多个副本中要保持强一致，比如前面说的分布式数据一致性问题。
- A：系统对外提供的服务必须一直处于可用状态，在任何故障下，客户端都能在合理的时间内获得服务端的非错误响应。
- P：在分布式系统中遇到任何网络分区故障，系统仍然能够过正常对外提供服务。

不同节点分布在不同的子网络中，在内部子网正常的情况下，**由于某些原因导致这些子节点之间出现网络不通的情况**
。导致整个系统环境被切分成若干独立的区域，这就是网络分区。

CAP定理证明，在分布式系统中，要么满足CP，要么满足AP，不可能实现CAP或者CA。原因是网络通信并不是绝对可靠的，比如网络延时、网络异常等都会导致系统故障。而在分布式系统中，即便出现网络故障也需要保证系统仍然能够正常对外提供服务，所以在分布式系统中Partition
Tolerance是必然存在的，也就是需要满足分区容错性。

如果是CA或者CAP这种情况，相当于网络百分之百可靠，否则当出现网络分区的情况时，为了保证数据的一致性，必须拒绝客户端的请求。但是如果拒绝了请求，就无法满足A，所以在分布式系统中不可能选择CA，因此只能有AP或者CP两种选择。

- AP：对于AP来说，相当于放弃了强一致性，实现最终的一致性，这是很多互联网公司解决分布式数据一致性问题的主要选择。
- CP：放弃了高可用性，实现强一致性。前面提到的两阶段提交和三阶段提交都采用这种方案。可能导致的问题是用户完成一个操作会等待较长的时间。

### BASE理论

**
BASE理论是由于CAP中一致性和可用性不可兼得而衍生出来的一种新的思想，BASE理论的核心思想是通过牺牲数据的强一致性来获得高可用性。**
它具有如下三个特性。

- Basically Available（基本可用）：分布式系统再出现故障时，允许损失一部分功能的可用性，保证核心功能的可用。
- Soft State（软状态）：允许系统中的数据存在中间状态，这个状态不影响系统的可用性，也就是允许系统中不同节点的数据副本之间的同步存在延时。
- Eventually Consistent（最终一致性）：中间状态的数据再经过一段时间后，会达到一个最终的数据一致性。

BASE理论并没有要求数据的强一致性，而是允许数据在一段时间内是不一致的，但是数据最终会在某个时间点实现一致。在互联网产品中，大部分都会采用BASE理论来实现数据的一致，因为产品的可用性对于用于来说更加重要。

举个例子，在电商平台中用户发起一个订单的支付，不需要同步等待支付的执行结果，**系统会返回一个支付处理中的状态到用户界面。**
对于用户来说，他可以从订单列表中看到支付的处理结果。而对于系统来说，当第三方的支付处理成功之后，在更新该订单的支付状态即可。在这个场景中，虽然订单的支付状态和第三方的支付状态存在短期的不一致，但是用户去获得了更高的产品体验。

# Paxos

于1990年提出的一种基于消息传递且具有**高度容错特性的共识（consensus）算法。**

**参考文章：**https://www.douban.com/note/208430424/?_i=8078357iO7DXeV

# Zookeeper

**ZooKeeper 是一个开源的分布式协调框架，是Apache Hadoop 的一个子项目，主要用来解决分布式集群中应用系统的一致性问题。Zookeeper
的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。ZooKeeper
是一个集中式服务，用于维护配置信息、命名、提供分布式同步和提供组服务。** 所有这些类型的服务都以某种形式被分布式应用程序使用。
每次实施它们时，都会进行大量工作来修复不可避免的错误和竞争条件。 由于实现这些服务的难度，应用程序最初通常会忽略它们，这使得它们在发生变化时变得脆弱并且难以管理。
即使正确完成，这些服务的不同实现也会在部署应用程序时导致管理复杂性。

ZooKeeper本质上是一个分布式的小文件存储系统（Zookeeper=文件系统+监听机制）。提供基于类似于文件系统的目录树方式的数据存储，并且可以对树中的节点进行有效管理，从而用来维护和监控存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理、统一命名服务、分布式配置管理、分布式消息队列、分布式锁、分布式协调等功能。

ZooKeeper
是一个开源的分布式协调框架,是一个集中式服务,本质上是一个分布式的小文件存储系统（Zookeeper=文件系统+监听机制）。提供基于类似于文件系统的目录树方式的数据存储，并且可以对树中的节点进行有效管理，从而用来维护和监控存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理、统一命名服务、分布式配置管理、分布式消息队列、分布式锁、分布式协调等功能。

## 命令

| 基本语法                                             | 功能描述                                                                                                               |
|--------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| help                                             | 显示所有操作命令                                                                                                           |
| ls [-s] [-w] [-R] path                           | 使用 ls 命令来查看当前 znode 的子节点 [可监听]  -w: 监听子节点变化 -s: 节点状态信息（时间戳、版本号、数据大小等）-R: 表示递归的获取                                   |
| create [-s] [-e] [-c] [-t ttl] path [data] [acl] | 创建节点-s : 创建有序节点。-e : 创建临时节点。-c : 创建一个容器节点。t ttl] : 创建一个TTL节点， -t 时间（单位毫秒）。data：节点的数据，可选，如果不使用时，节点数据就为null。acl：访问控制 |
| get [-s] [-w] path                               | 获取节点数据信息 -s: 节点状态信息（时间戳、版本号、数据大小等） -w: 监听节点变化                                                                      |
| set [-s] [-v version] path data                  | 设置节点数据-s:表示节点为顺序节点-v: 指定版本号                                                                                        |
| getAcl [-s] path                                 | 获取节点的访问控制信息-s: 节点状态信息（时间戳、版本号、数据大小等）                                                                               |
| setAcl [-s] [-v version] [-R] path acl           | 设置节点的访问控制列表-s:节点状态信息（时间戳、版本号、数据大小等）-v:指定版本号-R:递归的设置                                                                |
| stat [-w] path                                   | 查看节点状态信息                                                                                                           |
| delete [-v version] path                         | 删除某一节点，只能删除无子节点的节点。-v： 表示节点版本号                                                                                     |
| deleteall path                                   | 递归的删除某一节点及其子节点                                                                                                     |
| setquota -n\|-b val path                         | 对节点增加限制n:表示子节点的最大个数b:数据值的最大长度，-1表示无限制                                                                              |

## 作用

### 配置中心

可以发布数据到zookeeper，watch检测某些数据，当有数据变更的时候，会回掉客户端尽心数据的修改。

### 负载均衡

不咋使用利用一个目录下节点路径是唯一的，存储访问次数，达到访问比较好的次数，完全可以用nagix做用，反向代理。

### 分布式锁

也是利用同一目录下节点唯一性并配合watch可以达到一个有序队列的访问方式。

## 集群

- Leader： 领导者。

事务请求（写操作）的唯一调度者和处理者，保证集群事务处理的顺序性；集群内部各个服务器的调度者。对于create、setData、delete等有写操作的请求，则要统一转发给leader处理，leader需要决定编号、执行操作，这个过程称为事务。

- Follower: 跟随者

处理客户端非事务（读操作）请求（可以直接响应），转发事务请求给Leader；参与集群Leader选举投票。

- Observer: 观察者

对于非事务请求可以独立处理（读操作），对于事务性请求会转发给leader处理。Observer节点接收来自leader的inform信息，更新自己的本地存储，不参与提交和选举投票。通常在不影响集群事务处理能力的前提下提升集群的非事务处理能力。

**Zookeeper集群中节点之间数据是如何同步的？**

1、⾸先集群启动时，会先进⾏领导者选举，确定 哪个节点是Leader ，哪些节点是 Follower 和 Observer
2、然后Leader会和其他节点进⾏ 数据同步 ，采⽤ 发送快照 和 发送Diff⽇志 的⽅式
3、集群在⼯作过程中，所有的写请求都会交给Leader节点来进⾏处理，从节点只能处理读请求
4、Leader节点收到⼀个写请求时，会通过 两阶段机制 来处理
5、Leader节点会将该写请求对应的⽇志发送给其他Follower节点，并等待Follower节点持久化⽇志成功
6、Follower节点收到⽇志后会进⾏持久化，如果持久化成功则发送⼀个Ack给Leader节点
7、当Leader节点收到半数以上的 Ack 后，就会开始提交，先更新Leader节点本地的内存数据
8、然后发送 commit 命令给 Follower 节点，Follower节点收到commit命令后就会 更新各⾃本地内存数据
9、同时Leader节点还是将当前写请求直接发送给Observer节点，Observer节点收到Leader发过来的写请求后直接执⾏更新本地内存数据
10、最后Leader节点返回客户端写请求响应成功
11、通过 同步机制 和 两阶段提交机制 来达到 集群中节点数据⼀致