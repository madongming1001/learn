# SQL语句内部执行过程

MySQL分为**Server层**和**存储引擎层**两部分。Server层包括**连接器、查询缓存（默认关闭
have_query_cache，在一个表上有更新的时候，跟这个表有关的查询缓存会失效）、分析器**、**优化器**、**执行器**等等，而存储引擎层负责数据的存储和读取。
SQL执行时，会通过连接器**建立连接**、**获取权限**；连接器会**维持和管理连接**。 然后，MySQL会通过分析器对SQL语句进行**词法分析
**、语法分析，分析语句**各部分含义，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。
经过分析器分析后，MySQL会对SQL请求进行优化器的处理，优化器对语句索引、连接顺序等情况判断，决定使用哪种执行方案最合适。
最后，就到了执行器的阶段，执行器根据表的引擎定义，去调用引擎接口，执行SQL语句。

**并且`FORMAT`新增`TREE`格式**。通过EXPLAIN展示的信息我们可以了解到**表查询的顺序**，**表连接的方式**
等，并根据这些信息判断语句执行效率，决定是否添加索引或改写SQL语句优化表连接方式以提高执行效率。
EXPLAIN语法如下：

```sql
explain format='TREE' sql语句
```

## Explain（执行计划）

**Table 8.1 EXPLAIN Output Columns**

| 字段名           | 含义                   |
|:--------------|:---------------------|
| id            | 标识符，语句涉及表的执行顺序       |
| select_type   | 表查询类型                |
| table         | 表名称                  |
| partitions    | 涉及表哪个分区              |
| type          | 表的查询(连接)类型           |
| possible_keys | 表可能使用到的索引            |
| key           | 表实际使用到的索引            |
| key_len       | 表实际使用索引的长度，单位:字节     |
| ref           | 表哪些字段或者常量用于连接查找索引上的值 |
| rows          | 查询预估返回表的行数           |
| filtered      | 表经过条件过滤之后与总数的百分比     |
| Extra         | 额外的说明信息              |

explain 通过执行计划可以模拟优化器执行的sql语句，查询sql的可优化空间

set session optimizer_switch='derived_merge=off'; #关闭mysql5.7新特性对衍生表的合并优化
set session optimizer_switch='derived_merge=on'; #关闭mysql5.7新特性对衍生表的合并优化

1. **explain**：会在 explain 的基础上额外提供一些查询优化的信息。紧随其后通过 show warnings 命令可
   以得到优化后的查询语句，从而看出优化器优化了什么。额外还有 filtered 列，是一个半分比的值，rows filtered/100 可以**估算**
   出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的 表）
2. **explain partitions**：相比 explain 多了个 partitions 字段，如果查询是基于分区表的话，会显示查询将访问的分 区。

### **ID列**

Id列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。

### **Select_type列**

| select_type值         | 含义                             |
|:---------------------|:-------------------------------|
| SIMPLE               | 简单查询，不包含unino查询或子查询            |
| PRIMARY              | 位于最外部的查询                       |
| UNION                | 当出现union查询时第二个或之后的查询           |
| DEPENDENT UNION      | 当出现union查询时第二个或之后的查询，取决于外部查询   |
| UNION RESULT         | union查询的结果集                    |
| SUBQUERY             | 子查询当中第一个select查询               |
| DEPENDENT SUBQUERY   | 子查询当中第一个select查询，取决于外部的查询      |
| DERIVED              | 衍生表(FROM子句中的子查询)               |
| MATERIALIZED         | 物化子查询                          |
| UNCACHEABLE SUBQUERY | 结果集无法缓存的子查询，必须重新评估外部查询的每一行     |
| UNCACHEABLE UNION    | UNION中第二个或之后的SELECT，属于无法缓存的子查询 |

### **table列**

显示explain正在执行哪一张表

### partitions

该列显示的为分区表命中的分区情况。非分区表该字段为空（null）。

### type列

常见的

**system > const > eq_ref > ref > range > index > ALL**
一般来说，得保证查询达到range级别，最好达到ref

2. system,const：**表中只有一行数据或者是空表**，这是const类型的一个特例。且只能用于myisam和memory表。*
   *如果是Innodb引擎表，type列在这个情况通常都是all或者index**。
2. const :最多只有一行记录匹配。当联合主键或唯一索引的所有字段跟常量值比较时，join类型为const。其他数据库也叫做唯一索引扫描。
3. eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const
   之外最好的联接类型了，简单的 select 查询不会出现这种 type。
4. ref：对于来自前面表的每一行，在此表的索引中可以匹配到多行。相比
   eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。辅助索引
5. range：范围扫描通常出现在 in(), between ,> ,<, >= 等操作中。使用一个索引来检索给定范围的行。
6. index：扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用
   **覆盖索引**，二级索引一般比较小，所以这种通常比ALL快一些。
7. ALL：即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了。

### possible_keys列

显示了MySQL在查找当前表中数据的时候可能使用到的索引，实际意义不大。

### key列

实际使用的索引，如果为NULL，则没有使用索引。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 **force index**、**ignore index**。

### key_len列

这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。

显示了MySQL实际使用索引的大小，单位字节。可以通过key_len的大小判断评估复合索引使用了哪些部分。
几种常见字段类型索引长度大小如下，假设字符编码为utf8mb4：**如果字段允许为NULL，则需要额外增加一个字节**；
字符型：
char(n)：4n个字节（中文四字节，英文一个字节）
varchar(n)：4n+2个字节（中文四字节，英文一个字节）
数值型：
tinyint：1个字节
int：4个字节
bigint：8个字节
时间型：
date：3个字节
datetime：5个字节+秒精度字节 时间到9999年
timestamp：4个字节+秒精度字节 时间到2039年
秒精度字节(最大6位)：
1~2位：1个字节
3~4位：2个字节
5~6位：3个字节

索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引。

### ref列

这一列显示了在key列记录的索引中，**表查找值所用到的列或常量**，常见的有：const（常量），字段名（例：film.id）

### rows列

这是mysql估算的需要扫描的行数（不是精确值）。这个值非常直观显示 SQL的效率好坏, 原则上rows越少越好。

### filtered

这个字段表示存储引擎返回的数据在server层过滤后，**剩下多少满足查询的记录数量的比例**，注意是百分比，不是具体记录数。

### Extra列

这一列展示的是额外信息。常见的重要值如下：

- **Using index**仅查询索引树就可以获取到所需要的数据行，而不需要读取表中实际的数据行。通常适用于select字段就是查询使用索引的一部分，即使用了覆盖索引。

- **Using index condition**

   索引下推，部分where条件部分可以下推到存储引擎通过索引进行过滤，联合索引，当第一个条件匹配时，继续使用第二个条件在二级索引树上进行查询，减少回表的次数，大大提升了查询的效率。

  set optimizer_switch='index_condition_pushdown=off';

  set optimizer_switch='index_condition_pushdown=on';默认开启

- **Using where**
  显示MySQL通过索引条件定位之后还需要返回表中获得所需要的数据。
- **Impossible WHERE**
  where子句的条件永远都不可能为真。

- **Using join buffer** (Block Nested Loop), Using join buffer (Batched Key Access)
  在表联接过程当中，将先前表的部分数据读取到join buffer缓冲区中，然后从缓冲区中读取数据与当前表进行连接。主要有两种算法：Block
  Nested Loop和Batched Key Access。8.0取消了BlockNested loop，只有Nested loop join和hash join

  ```sql
  SET PERSIST optimizer_switch='mrr=on,mrr_cost_based=off,block_nested_loop=on,batched_key_access=on/off';
  ```

- **Using MRR**
  读取数据采用多范围读(Multi-Range Read)的优化策略。

- **Using temporary**
  MySQL需要创建临时表来存放查询结果集。通常发生在有**GROUP BY**或**ORDER BY**子句的语句当中。

- **Using filesort**
  MySQL需要对获取的数据进行额外的一次排序操作，无法通过索引的排序完成。通常发生在有ORDER BY子句的语句当中。

**mysql8.0执行计划 参考文章：**https://developer.aliyun.com/article/712421

# InnoDB行记录格式

## COMPACT

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4beb83ce7efa4ed99596da1f82241e33~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

## Redundant

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d0e85ebd3593473eb497f838c1cb038c~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

## COMPRESSED 和 DYNAMIC

Compressed 和 Dynamic 行记录格式与 Compact 行记录格式是类似的，只不过在处理行溢出数据时有些区别。

这两种格式采用完全的行溢出方式，数据页不会存储真实数据的前768字节，只存储20个字节的指针来指向溢出页。而实际的数据都存储在溢出页中，看起来就像下面这样：

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/76ef672336474d93b309077f7df324c4~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

Compressed 与 Dynamic 相比，Compressed 存储的行数据会以zlib的算法进行压缩以节省空间，因此对于 BLOB、TEXT、VARCHAR
这类大长度类型的数据能够进行非常有效的存储。

## VARCHAR类型

一行数据，除了 TEXT、BLOB 等大对象类型，总长度最大 `65535字节`。而且这个 65535
最大长度是包含 `变长字段长度列表`、`NULL值列表` 的。

`VARCHAR(M)` 中的 M 指的是字符长度，而不是字节长度，计算时，要用总长度除以字符集最大长度，例如 utf8mb4 字符集每个字符的最大长度为
4字节。

VARCHAR 类型如果小于255字节，要在变长字段长度列表占 `1字节`，否则占 `2字节`；如果可为NULL，还要在NULL值列表占 `1字节`
，不过这一个字节可以存8个可为NULL的列的状态。所以一个 VARCHAR(M) 的字节长度最大为 `65532字节`。

## CHAR 数据类型

我们一般会认为 `CHAR(M)` 是定长类型，M 与 VARCHAR(M) 中的 M 是一样的，**指的是字符的长度**。类型为CHAR(M)时，*
*对于长度不足的值会用空格来补足，就算存的是空值，也会用空格补足，查询的时候会去除首尾的空格，而VARCHAR就不会。**

## 行溢出数据

MySQL中磁盘和内存交互的基本单位是`页`，一个页的大小一般是`16KB`，也就是`16384字节`，而一个VARCHAR(M)
类型的列最多可以存储`65532字节`，一些大对象如 TEXT、BLOB
可能存储更多的数据，这时一个页可能就存不了一条记录。这个时候就会发生`行溢出`，多的数据就会存到另外的`溢出页`中。

InnoDB 规定一页至少存储两条记录，如果页中只能存放下一条记录，InnoDB存储引擎会自动将行数据存放到溢出页中。在一般情况下，InnoDB
的数据都是存放在 `FIL_PAGE_INDEX` 类型的数据页中的。但是当发生行溢出时，溢出的数据会存放到 `FIL_PAGE_TYPE_BLOB` 类型的溢出页中。

当发生行溢出时，**数据页只保存了前768字节的前缀数据**，接着是20个字节的偏移量，指向行溢出页，大致如下图所示。

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/20da3f98a3ce4a2d89561ee7feff7e81~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

**参考文章：**https://juejin.cn/post/6970934163973079048

# InnoDB索引页格式

 前边我们简单提了一下页的概念，它是 InnoDB 管理存储空间的基本单位，一个页的大小一般是 16KB。

 InnoDB 为了不同的目的而设计了许多种不同类型的页，存放我们表中记录的那种类型的页自然也是其中的一员，官方称这种存放记录的页为
**索引（INDEX）页**，不过要理解成数据页也没问题，毕竟存在着聚簇索引这种索引和数据混合的东西。

![image-20220208190217690](noteImg/image-20220208190217690.png)

**Mysql 8.0 memory structures**

![MySQL8.0官方文档学习_MySQL8.0](https://s2.51cto.com/images/blog/202104/25/87eb9cd0e6e435fe9015fafcdb2961be.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

**区（**extent**）**

 表空间中的页可以达到 2^32个页，实在是太多了，为了更好的管理这些页面，InnoDB 中还有一个区（英文名：extent）的概念。对于 16KB
的页来说，连续的64 个页就是一个区，也就是说一个区默认占用 1MB 空间大小。通常会按照区来申请连续的磁盘空间

**组**（group）

 为了管理大量的区每 256个区又被划分成一个**区组**

**段**（segment）

**逻辑上的概念，本质上是由若干个零散页面和若干个完整的区组成的。**主要管理索引树中的叶子、非叶子结点

# Innodb三大特性

### doublewrite

​        **doublewrite是为了保证数据页的可靠性。避免发生部分写失效（partial page write）**
，当发生数据库宕机时，可能InnoDB存储引擎正在写入某个页到表中，而这个页只写了一部分，比如16kb的页，只写了前4kb，就发生了宕机，这种情况被称为部分写失效（partial
page write）

Double Write 分为了两个组成部分：

- 内存中的double write buffe 2M
- 物理磁盘上共享表空间中连续的128个页，即2个区（extent），大小同样为2MB

**表空间**

- **共享表空间**，来自多个不同表及其对应索引的数据可以保存在单个.ibd文件中。
- **独立表空间**，单个表的数据及其索引保存在 一个.ibd文件中。

因为存储引擎缓冲池内的数据页大小默认为16KB，我们都知道 Linux
会以页为单位管理[内存](https://so.csdn.net/so/search?q=内存&spm=1001.2101.3001.7020)
，无论是将磁盘中的数据加载到内存中，还是将内存中的数据写回磁盘，操作系统都会以页面为单位进行操作，哪怕我们只向磁盘中写入一个字节的数据，我们也需要将整个页面中的全部数据刷入磁盘中。

**内核把物理页作为内存管理的基本单位**
。尽管处理器的最小可寻址单位通常为字（甚至字节），但是，内存管理单元（MMU，管理内存并把虚拟内存地址转换为物理地址的硬件）通常以页为单位进行处理。正因为如此，MMU以页（page）大小为单位来管理系统中的页表（这也是页表名的来由）。从虚拟内存的角度来看，页就是最小单位。

所以在进行刷盘操作时，就有可能发生如下场景：

![img](https://ask.qcloudimg.com/http-save/7768581/bli4l4gekd.jpeg?imageView2/2/w/1620)

由于重做日志redo log记录的是物理内容，比如那个页那个offset修改了什么数据，一旦数据页被破坏了redo log就作用不了了，所以在应用重做日志前，
**用户需要一个页的副本，当写入失效发生时，先通过页的副本来还原该页。**

写入过程，在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是会通过memcpy函数将页先复制到内存中的doublewrite
buffer，之后通过doublewrite
buffer在分两次，每次1mb顺序地写入共享表空间的物理磁盘上，然后马上调用fsync函数，同步磁盘，避免缓冲写带来的问题。在这个过程中，因为doublewrite页是连续的，因此这个过程时顺序写，开销并不是很大。在完成doublewrite页的写入后，再将doublewriitebuffer中的页写入各个表空间文件中，此时的写入时离散的。

**Double write崩溃恢复**

![img](https://ask.qcloudimg.com/http-save/7768581/knfsxb02gp.jpeg?imageView2/2/w/1620)

参考文章：https://cloud.tencent.com/developer/article/1739772

如果操作系统在将页写入磁盘的过程中发生了崩溃，再恢复过程中，InnoDB存储引擎可以从共享表空间中的doublewrite中找到该页的一个副本，将其复制到表空间文件，再应用重做日志。

### Buffer Pool

 我们知道，对于使用 InnoDB 作为存储引擎的表来说，不管是用于存储用户数据的索引（包括聚簇索引和二级索引），还是各种系统数据，*
*都是以页的形式存放在表空间中的**，而所谓的表空间只不过是 InnoDB 对文件系统上一个或几个实际文件的抽象，也就是说我们的数据说到底还是存储在磁盘上的。但是磁盘的速度慢，所以
InnoDB
存储引擎在处理客户端的请求时，当需要访问某个页的数据时，就会把完整的页的数据全部加载到内存中，也就是说即使我们只需要访问一个页的一条记录，那也需要先把整个页的数据加载到内存中。将整个页加载到内存中后就可以进行读写访问了，在进行完读写访问之后并不着急把该页对应的内存空间释放掉，而是将其缓存起来，这样将来有请求再次访问该页面时，就可以省去磁盘
IO 的开销了。

**free链表**

 buffer存储格式都是按照每个存储页都对应一个控制块而所有的控制块在前，存储页在后，如果新来一个页如何辨别哪里是空位置可以存放呢，就用到了
**free链表**会把空闲的所有控制块都放入到**free链表**

![image-20220208201845717](noteImg/image-20220208201845717.png)

我们其实是根据表空间号 + 页号来定位一个页的，所以我们可以用表空间号 + 页号作为 key，缓存页作为 value 创建一个**哈希表**
，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从
free 链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。

**flush链表**

 如果我们修改了 Buffer Pool 中某个缓存页的数据，那它就和磁盘上的页不一致了，这样的缓存页也被称为脏页（英文名：dirty
page）所以，需要再创建一个存储脏页的链表，凡是修改过的缓存页对应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的，所以也叫
flush 链表。

 如果非常多的使用频率偏低的页被同时加载到 Buffer Pool 时，可能会把那些使用频率非常高的页从 Buffer Pool 中淘汰掉。

**刷新脏页到磁盘**

后台有专门的线程每隔一段时间负责把脏页刷新到磁盘，这样可以不影响用户线程处理正常的请求。主要有两种刷新路径：

1、从 LRU 链表的冷数据中刷新一部分页面到磁盘。

2、从 flush 链表中刷新一部分页面到磁盘。

**LRU优化**

因为有这两种情况的存在，所以 InnoDB 把这个 LRU 链表按照一定比例分成两截，

分别是：

一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做热数据，或者称 young 区域。

另一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做冷数据，或者称 old 区域。

![image-20220208203449226](noteImg/image-20220208203449226.png)

所以在对某个处在 old 区域的缓存页进行第一次访问时就在它对应的控制块中记录下来这个访问时间，*
*如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从 old 区域移动到 young 区域的头部**，否则将它移动到
young 区域的头部。上述的这个间隔时间是由系统变量innodb_old_blocks_time 控制的：

![image-20220208203815265](noteImg/image-20220208203815265.png)

这个 innodb_old_blocks_time 的默认值是 1000，它的单位是毫秒，也就意味着对于从磁盘上被加载到 LRU 链表的 old
区域的某个页来说，如果第一次和最后一次访问该页面的时间间隔小于 1s（很明显在一次全表扫描的过程中，多次访问一个页面中的时间不会超过
1s），那么该页是不会被加入到 young 区域的，当然，像innodb_old_blocks_pct 一样，我们也可以在服务器启动或运行时设置innodb_old_blocks_time
的值，这里需要注意的是，如果我们把innodb_old_blocks_time 的值设置为 0，那么每次我们访问一个页面时就会把该页面放到 young
区域的头部。

​        **当从磁盘读取数据页后，会先将数据页存放到 LRU 链表冷数据区的头部，**如果这些缓存页在 1
秒之后被访问，那么就将缓存页移动到热数据区的头部；如果是 1 秒之内被访问，则不会移动，缓存页仍然处于冷数据区中。1 秒这个数值，是由参数
**innodb_old_blocks_time** 控制。

 当遇到全表扫描或者预读时，如果没有空闲缓存页来存放它们，那么将会淘汰一个数据页，**而此时淘汰地是冷数据区尾部的数据页**
。冷数据区的数据就是不经常访问的，因此这解决了误将热点数据淘汰的问题。如果在 1
秒后，因全表扫描和预读机制额外加载进来的缓存页，仍然没有人访问，那么它们会一直待在冷数据区，当再需要淘汰数据时，首先淘汰地就是这一部分数据。

至此，基于冷热分离优化后的 LRU 链表，完美解决了直接使用 LRU 链表带来的问题。

 综上所述，正是因为将 LRU 链表划分为 young 和 old 区域这两个部分，又添加了 **innodb_old_blocks_time** 这个系统变量，才*
*使得预读机制和全表扫描造成的缓存命中率降低的问题得到了遏制**，因为用不到的预读页面以及全表扫描的页面都只会被放到 old
区域，而不影响 young 区域中的缓存页。

**LRU 链表的极致优化**

把young区域等分为4份，当处于前4分之1的时候不需要移动到young头部，后4分之三需要，优化目的在于**减少对于同步所带来的开销损耗。
**

实际上，MySQL 在冷热分离的基础上还做了一层优化。

当一个缓存页处于热数据区域的时候，我们去访问这个缓存页，这个时候我们真的有必要把它移动到热点数据区域的头部吗？

从代码的角度来看，将链表中的数据移动到头部，实际上就是修改元素的指针指向，这个操作是非常快的。但是为了安全起见，在修改链表的时候，我们需要对链表加上锁，否则容易出现并发问题。

当并发量大的时候，因为要加锁，会存在锁竞争，每次移动显然效率就会下降。因此 MySQL 针对这一点又做了优化，如果一个缓存页处于热数据区域，且在热数据区域的前
1/4 区域（注意是热数据区域的 1/4，不是整个链表的 1/4），那么当访问这个缓存页的时候，就不用把它移动到热数据区域的头部；如果缓存页处于热数据的后
3/4 区域，那么当访问这个缓存页的时候，会把它移动到热数据区域的头部。

**生产上的 MySQL 调优**

MySQL 的数据最终是存储在磁盘上的，每次查询数据时，我们先需要把数据加载进缓存，然后读取，如果每次查询的数据都已经存在于缓存了，那么就不用去磁盘读取，避免了一次磁盘
IO，这是我们最期望的。因此为了尽量在 LRU 链表中缓存更多的缓存页，我们**「可以根据服务器的配置，尽量调大 Buffer Pool
的大小」**。

另外在实际应用中，在没有外部监控工具的情况下，我们该如何知道 MySQL 的一些状态信息呢？如：缓存命中率、缓存页的空闲数、脏页数量、LRU
链表中缓存页个数、冷热数据的比例、磁盘 IO 读取的数据页数量等信息。可以通过如下命令查看：

```text
show engine innodb status;
```

这个命令的查询结果是一个很长的字符串，可以复制出来，放在文本文件中查看分析，部分信息截图如下：

![img](https://pic4.zhimg.com/80/v2-68e305fe88d5034c80de717c3a56ba87_720w.webp)

如果看到 **「youngs/s」** 这个值较高，说明数据从冷数据区移到热数据的频率较大，因此可以适当调大热数据所占的比例，也就是减小**
「innodb_old_blocks_pct」**参数的值，也可以调大**「innodb_old_blocks_time」**参数的值

**参考文章：https://zhuanlan.zhihu.com/p/142087506**

### 自适应哈希索引


InnoDB存储引擎除了我们前面所说的各种索引，还有一种自适应哈希索引，我们知道B+树的查找次数,取决于B+树的高度,在生产环境中,B+树的高度一般为3~
4层,故 需要3~4次的IO查询。

​        **InnoDB存储引擎会监控对表上各索引页的查询**。如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引（Adaptive
Hash Index，AHI）。AHI是通过缓冲池的B+树页构造而来，因此建立的速度很快，而且不需要对整张表构建哈希索引。*
*InnoDB存储引擎会自动根据访问的频率和模式来自动地为某些热点页建立哈希索引。**

 AHI有一个要求，即对这个页的连续访问模式必须是一样的。例如对于（a，b）这样的联合索引页，其访问模式可以是以下的情况：

 where a = xxx

 where a = xxx && b = xxx

访问模式一样指的是查询的条件一样，交替也不行，模式访问次数，这个页通过模式访问了多少次。提升是很大的，有一个条件必须是等值查询。通过
**show engine innodb status**可以查看使用的结果。**innodb_adaptive_hash_index**可以禁用

### 预读

 InnoDB 提供了预读（英文名：read ahead）。所谓预读，就是 InnoDB认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到
Buffer Pool中。根据触发方式的不同，预读又可以细分为下边两种：

***线性预读***

InnoDB 提供了一个系统变量 **innodb_read_ahead_threshold**，如果顺序访问了某个区（extent）的页面超过这个系统变量的值，*
*就会触发一次异步读取下一个区中全部的页面到 Buffer Pool 的请求**。这个 innodb_read_ahead_threshold 系统变量的值默认是
56，我们可以在服务器启动时通过启动参数或者服务器运行过程中直接调整该系统变量的值，取值范围是 0~64。

***随机预读***

如果 Buffer Pool 中已经缓存了某个区的 13 个连续的页面，不论这些页面是不是顺序读取的，都会触发一次异步读取本区中所有其他的页面到
Buffer Pool 的请求。InnoDB同时提供了innodb_random_read_ahead 系统变量，它的默认值为OFF。

**show variables like '%_read_ahead%';**

# 数据库范式

目前关系数据库有六种范式：第一范式（

1NF）、第二范式（2NF）、第三范式（3NF）、 巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。

**数据库设计的第一范式**

定义： 属于第一范式关系的所有属性都不可再分，即数据项不可分。

![image-20220208213231317](noteImg/image-20220208213231317.png)

**数据库设计的第二范式**

第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。通常在实现来

说，需要为表加上一个列，以存储各个实例的惟一标识。主键ID。

**数据库设计的第三范式**

指每一个非主属性既不部分依赖于也不传递依赖于业务主键，也就是在第二范式的基础 上消除了非主键对主键的传递依赖。*
*不包含已在其它表中已包含的非主关键字信息。**

### **反范式设计**

 所谓得反范式化就是为了性能和读取效率得考虑而适当得对数据库设计范式得要求进行 违反。允许存在少量得冗余，换句话来说反范式化就是使用空间来换取时间。

### 三星索引

1.索引将相关的记录放到一起则获得一星 离得越近比如在同一个页，**索引的扫描范围越小**

2.如果索引中的数据顺序和查找的排序顺序一致则获得二星（排序星）**有点类似于最左匹配完全**

3.如果索引中的列包含了查询中需要的全部列则获得三星（宽索引星）**可以理解为覆盖索引**

## Mysql内部优化策略

1、移除不必要的括号

2、常量传递（constant_propagation）

3、移除没用的条件（trivial_condition_removal）

4、表达式计算

## Mysql性能优化

1、硬件优化

2、Mysql调优

1. 业务层-请求了不需要的数据
2. 选择合适的存储引擎 Innodb MyISAM
3. 查询性能优化 通过慢查询日志 slow_query_log
4. 根据相爱年供应时间

3、架构调优

# Mysql组件

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/0d2070e8f84c4801adbfa03bda1f98d9.png)

## 连接器

**管理连接与权限校验**

客户端如果长时间不发送command到Server端，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值 是 8 小时。
28800秒

show global variables like "wait_timeout";

Lost connection to MySQL server during query 查询期间丢失与 MySQL 服务器的连接

开发当中我们大多数时候用的都是长连接,把连接放在Pool内进行管理，但是长连接有些时候会导致 MySQL 占用内存涨得特别快，这是因为
MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是
MySQL 异常重启了。

**怎么解决这类问题呢？**

1、定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。

2、如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行**mysql_reset_connection**
来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

**查询缓存（mysql8.0已经移除了查询缓存功能）**

查询缓存按照理想来说是对于效率提升很好的一个手段，但是由于缓存失效的非常频繁，只要对一个表的更新，那这张表的所有缓存都会被清空一般也就长时间不被修改的表才用到缓存。而对于8.0之前有三个参数来决定query_cache_type缓存的使用

0代表关闭查询缓存

1代表开启缓存

2代表只有遇到关键字sql_cache关键字时才缓存

## 分析器

如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。

分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。

MySQL 从你输入的"select"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。

做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL
语法。

如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。

```vbnet
mysql> elect * from t where ID=1;
 
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1
```

一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。

mysql的词法分析由MysqlLex（mysql自己实现的），语法分析由Bison生成。

## 优化器

执行计划生成，索引选择

经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的
join：

```csharp
mysql> select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;
```

- 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。
- 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。

这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，没关系，我会在后面的文章中单独展开说明优化器的内容。

## 执行器

MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (
在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。

```sql
mysql> select * from T where ID=10;
 
ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T'
```

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：

1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
    1. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

**索引使用情况（mysql5.6引入索引下推）**

1、联合索引第一个字段用范围不会走索引

2、强制走索引 force index（） 忽略走索引 ignore index

3、覆盖索引优化

4、in和or在表数据量比较大的情况会走索引，在表记录不多的情况下会选择全表扫描

5、like KK% 一般情况都会走索引

 索引下推（Index Condition Pushdown）可以在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可
以有效的减少回表次数。

**为什么范围查找Mysql没有用索引下推优化？*

估计应该是Mysql认为**范围查找过滤的结果集过大**，like KK% 在绝大多数情况来看，过滤后的结果集比较小，所以这里Mysql选择给
like

KK% 用了索引下推优化，当然这也不是绝对的，有时like KK% 也不一定就会走索引下推。

order by 与group by

order by根据where和order by字段的情况 再根据联合索引或者其他耳机索引的创建情况决定是否走了index和filesort

排序方式也是决定一部分因素之一

```mysql
explain select from employees where name in ('LiLei','zhuge')  order by age,position;
```

对于排序来说多个相等条件也是范围查询

1、MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index

效率高，filesort效率低。

2、order by满足两种情况会使用Using index。

 1) order by语句使用索引最左前列。

 2) 使用where子句与order by子句条件列组合满足索引最左前列。

3、尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最左前缀法则。

4、如果order by的条件不在索引列上，就会产生Using filesort。

5、能用覆盖索引尽量用覆盖索引

6、group by与order by很类似，其实质是先排序后分组，遵照索引创建顺序的最左前缀法则。对于group

by的优化如果不需要排序的可以加上**order by null禁止排序**。注意，where高于having，能写在where中

的限定条件就不要去having限定了。

**索引原则**

1、代码先行，索引后上

2、联合索引尽量覆盖条件

3、不要在小基数字段上建立索引

4、长字符串我们可以采用前缀索引

5、where与order by冲突时优先where

6、基于慢sql查询做优化

### sql 慢查询

MySQL的慢查询，全名是**慢查询日志**，是MySQL提供的一种日志记录，用来记录在MySQL中**响应时间超过阀值**的语句。

long_query_time（默认是10秒）

![image-20211111142906400](noteImg/image-20211111142906400.png)

设置完成后需要重启数据库 设置方式**https://blog.csdn.net/qq_40884473/article/details/89455740**

`set global slow_query_log=1`开启了慢查询日志只对当前数据库生效，MySQL重启后则会失效。

关于运行时间**正好等于**`long_query_time`的情况，并不会被记录下来。

##### log-queries-not-using-indexes

该系统变量指定**未使用索引的查询**也被记录到慢查询日志中（可选项）。

##### log_slow_admin_statements

这个系统变量表示，是否将慢管理语句例如`ANALYZE TABLE`和`ALTER TABLE`等记入慢查询日志。

##### Slow_queries

如果你想查询有多少条慢查询记录，可以使用`Slow_queries`系统变量。

##### mysqldumpslow工具

在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活。

MySQL提供了日志分析工具`mysqldumpslow`

# **join算法**

**
参考文章：**http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/35%20%20join%E8%AF%AD%E5%8F%A5%E6%80%8E%E4%B9%88%E4%BC%98%E5%8C%96%EF%BC%9F.md

**
参考文章：**https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%98%E5%AE%9D%E5%85%B8/12%20%20JOIN%20%E8%BF%9E%E6%8E%A5%EF%BC%9A%E5%88%B0%E5%BA%95%E8%83%BD%E4%B8%8D%E8%83%BD%E5%86%99%20JOIN%EF%BC%9F.md

## Nested Loop Join 算法

**拿驱动表的条件索引去被驱动表根据索引找就叫做Index Nested-Loop Join**

我们来看一下这个语句：

```csharp
select * from t1 straight_join t2 on (t1.a=t2.a);
```

如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表，这样会影响我们分析 SQL
语句的执行过程。所以，为了便于分析执行过程中的性能问题，我改用 straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去
join。在这个语句里，t1 是驱动表，t2 是被驱动表。

在这条语句里，被驱动表 t2 的字段 a 上有索引，join 过程用上了这个索引，因此这个语句的执行流程是这样的：

1. 从表 t1 中读入一行数据 R；
2. 从数据行 R 中，取出 a 字段到表 t2 里去查找；
3. 取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分；
4. 重复执行步骤 1 到 3，直到表 t1 的末尾循环结束。

这个过程是先遍历表 t1，然后根据从表 t1 中取出的每行数据中的 a 值，去表 t2
中查找满足条件的记录。在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index
Nested-Loop Join”，简称 NLJ。

## Block Nested Loop Join

被驱动表上没有可用的索引，算法的流程是这样的：

1. 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存；
2. 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。

![image-20221026174422681](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20221026174422681.png)

可以看到，在这个过程中，对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表
t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。

然后，你可能马上就会问了，这个例子里表 t1 才 100 行，要是表 t1 是一个大表，join_buffer 放不下怎么办呢？join_buffer 的大小是由参数
join_buffer_size 设定的，默认值是 256k。**如果放不下表 t1 的所有数据话，策略很简单，就是分段放**。我把 join_buffer_size 改成
1200，再执行：

```sql
select * from t1 straight_join t2 on (t1.a=t2.b);
```

执行过程就变成了：

1. 扫描表 t1，顺序读取数据行放入 join_buffer 中，放完第 88 行 join_buffer 满了，继续第 2 步
2. 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回；
3. 清空 join_buffer；
4. 继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。

**第一个问题：能不能使用 join 语句？**

1. 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的；
2. 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种
   join 尽量不要用。

所以你在判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。

**第二个问题是：如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？**

1. 如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表；
2. 如果是 Block Nested-Loop Join 算法：
    - 在 join_buffer_size 足够大的时候，是一样的；
    - 在 join_buffer_size 不够大的时候（这种情况更常见），应该选择小表做驱动表。

**对于小表的定义在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join
的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2
作为驱动表**Multi-Range Read 优化

参考文章：http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/35%20%20join%E8%AF%AD%E5%8F%A5%E6%80%8E%E4%B9%88%E4%BC%98%E5%8C%96%EF%BC%9F.md

## Multi-Range Read 优化 (MRR)

**这个优化的主要目的是尽量使用顺序读盘。如果数据已经加载在bufferpool中，那么MRR主要是为了减少这个回表的次数，如果数据在bufferpool中不存在，那么不仅仅减少了回表的次数，同时也减少了随机IO，减少磁盘的交互数
**

```sql
create table t1(id int primary key, a int, b int, index(a));
select** * **from** t1 **where** a>=1 **and** a<=100;
```

**因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。
**

这，就是 MRR 优化的设计思路。此时，语句的执行流程变成了这样：

1. 根据索引 a，定位到满足条件的记录，将 id 值放入 **read_rnd_buffer** 中 ;
2. 将 **read_rnd_buffer** 中的 id 进行递增排序；
3. 排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。

```sql
show variables like '%read_rnd_buffer_size%';
```

**另外需要说明的是，如果你想要稳定地使用 MRR 优化的话，需要设置`set optimizer_switch="mrr_cost_based=off"`
。（官方文档的说法，是现在的优化器策略，判断消耗的时候，会更倾向于不使用 MRR，把 mrr_cost_based 设置为 off，就是固定使用 MRR
了。）**8.0的时候默认开启

**MRR 能够提升性能的核心**在于，这条查询语句在索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键
id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。

## Batched Key Access （8.0禁用）

理解了 MRR 性能提升的原理，我们就能理解 MySQL 在 5.6 版本后开始引入的 Batched Key Access(BKA) 算法了。这个 BKA 算法，其实就是对
NLJ 算法的优化。

NLJ 算法执行的逻辑是：从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。也就是说，对于表 t2 来说，每次都是匹配一个值。这时，MRR
的优势就用不上了。

那怎么才能一次性地多传些值给表 t2 呢？方法就是，从表 t1 里一次性地多拿些行出来，一起传给表 t2。

既然如此，我们就把表 t1 的数据取出来一部分，先放到一个临时内存。这个临时内存不是别人，就是 **join_buffer**。

通过上一篇文章，我们知道 join_buffer 在 BNL 算法里的作用，是暂存驱动表的数据。但是在 NLJ 算法里并没有用。那么，我们刚好就可以复用
join_buffer 到 BKA 算法中。

图中，我在 join_buffer 中放入的数据是 P1~P100，表示的是只会取查询需要的字段。当然，如果 join buffer 放不下 P1~P100
的所有数据，就会把这 100 行数据分成多段执行上图的流程。

那么，这个 BKA 算法到底要怎么启用呢？

如果要使用 BKA 优化算法的话，你需要在执行 SQL 语句之前，先设置

```bash
set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';
```

其中，前两个参数的作用是要启用 MRR。这么做的原因是，BKA 算法的优化要依赖于 MRR。理解了 MRR 性能提升的原理，我们就能理解 MySQL
在 **5.6 版本后**开始引入的 Batched Key Access(BKA) 算法了。这个 BKA 算法，其实就是对 NLJ 算法的优化。 8.0默认关闭

```sql
show variables like '%optimizer_switch%';
```

**问题：查询虚拟列会导致sql显示不了**

**参考文章：**https://blog.csdn.net/bczzm/article/details/100577819

## Block Nested Loop 算法的性能问题

使用 Block Nested-Loop Join(BNL) 算法时，可能会对被驱动表做多次扫描。如果这个被驱动表是一个大的冷数据表，除了会导致 IO
压力大以外，还会对系统有什么影响呢？

我们说到 InnoDB 的 LRU 算法的时候提到，由于 InnoDB 对 Bufffer Pool 的 LRU 算法做了优化，即：第一次从磁盘读入内存的数据页，会先放在
old 区域。如果 1 秒之后这个数据页不再被访问了，就不会被移动到 LRU 链表头部，这样对 Buffer Pool 的命中率影响就不大。

但是，如果一个使用 BNL 算法的 join 语句，多次扫描一个冷表，而且这个语句执行时间超过 1 秒，就会在再次扫描冷表的时候，把冷表的数据页移到
LRU 链表头部。

**出现两种问题：**

1. 如果冷表数据能完全融入Buffer Pool old区的话，由于多次扫描冷表导致sql执行时间超过1秒，就会把冷表的数据页移到LRU链表头部。
2. 如果这个冷表很大，就会出现另外一种情况：业务正常访问的数据页，没有机会进入 young 区域。

由于优化机制的存在，一个正常访问的数据页，要进入 young 区域，需要隔 1 秒后再次被访问到。但是，由于我们的 join 语句在*
*循环读磁盘和淘汰内存页**，进入 old 区域的数据页，很可能在 1 秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool
在这段时间内，young 区域的数据页没有被合理地淘汰。

也就是说，这两种情况都会影响 Buffer Pool 的正常运作。

**大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO 的影响也就结束了。但是，对 Buffer Pool
的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。**

为了减少这种影响，你可以考虑**增大 join_buffer_size 的值**，减少对被驱动表的扫描次数。或者调整进入热表的时间 调整为5秒什么的

也就是说，BNL 算法对系统的影响主要包括三个方面：

1. 可能会多次扫描被驱动表，占用磁盘 IO 资源；
2. 判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源；
3. 可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。

我们执行语句之前，需要通过理论分析和查看 explain 结果的方式，确认是否要使用 BNL 算法。如果确认优化器会使用 BNL
算法，就需要做优化。优化的常见做法是，给被驱动表的 join 字段加上索引，把 BNL 算法转成 BKA 算法。

## Block Nested Loop 转 Batched Key Access

一些情况下，我们可以直接在被驱动表上建索引，这时就可以直接转成 BKA 算法了。

但是，有时候你确实会碰到一些不适合在被驱动表上建索引的情况。比如下面这个语句：

```csharp
select * from t1 join t2 on (t1.b=t2.b) where t2.b>=1 and t2.b<=2000;
```

我们在文章开始的时候，在表 t2 中插入了 100 万行数据，但是经过 where 条件过滤后，需要参与 join 的只有 2000
行数据。如果这条语句同时是一个低频的 SQL 语句，那么再为这个语句在表 t2 的字段 b 上创建一个索引就很浪费了。

但是，如果使用 BNL 算法来 join 的话，这个语句的执行流程是这样的：

1. 把表 t1 的所有字段取出来，存入 join_buffer 中。这个表只有 1000 行，join_buffer_size 默认值是 256k，可以完全存入。
2. 扫描表 t2，取出每一行数据跟 join_buffer 中的数据进行对比，
    - 如果不满足 t1.b=t2.b，则跳过；
    - 如果满足 t1.b=t2.b, 再判断其他条件，也就是是否满足 t2.b 处于 [1,2000] 的条件，如果是，就作为结果集的一部分返回，否则跳过。

我在上一篇文章中说过，对于表 t2 的每一行，判断 join 是否满足的时候，都需要遍历 join_buffer 中的所有行。因此判断等值条件的次数是
1000*100 万 =10 亿次，这个判断的工作量很大。

可以看到，explain 结果里 Extra 字段显示使用了 BNL 算法。在我的测试环境里，这条语句需要执行 1 分 11 秒。

在表 t2 的字段 b 上创建索引会浪费资源，但是不创建索引的话这个语句的等值条件要判断 10 亿次，想想也是浪费。那么，有没有两全其美的办法呢？

这时候，我们可以考虑使用临时表。使用临时表的大致思路是：

1. 把表 t2 中满足条件的数据放在临时表 tmp_t 中；
2. 为了让 join 使用 Batched Key Access 算法，给临时表 tmp_t 的字段 b 加上索引；
3. 让表 t1 和 tmp_t 做 join 操作。

此时，对应的 SQL 语句的写法如下：

```sql
create temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb;
insert into temp_t select * from t2 where b>=1 and b<=2000;
select * from t1 join temp_t on (t1.b=temp_t.b);
```

可以看到，整个过程 3 个语句执行时间的总和还不到 1 秒，相比于前面的 1 分 11 秒，性能得到了大幅提升。接下来，我们一起看一下这个过程的消耗：

1. 执行 insert 语句构造 temp_t 表并插入数据的过程中，对表 t2 做了全表扫描，这里扫描行数是 100 万。
2. 之后的 join 语句，扫描表 t1，这里的扫描行数是 1000；join 比较过程中，做了 1000 次带索引的查询。相比于优化前的 join 语句需要做
   10 亿次条件判断来说，这个优化效果还是很明显的。

总体来看，**不论是在原表上加索引**，**还是用有索引的临时表**，我们的思路都是让 join 语句能够用上被驱动表上的索引，来触发 BKA
算法，提升查询性能。

# order原理

## 全字段排序

参考文章：http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/16%20%20%E2%80%9Corder%20by%E2%80%9D%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9F.md

**先按照索引把所需要的所有字段数据放入到sort_buffer,然后对 sort_buffer 中的数据按照字段 name 做归并排序，按照排序结果返回给客户端。
**

**通常情况下，这个语句执行流程如下所示 ：**

1. 初始化 sort_buffer，确定放入 name、city、age 这三个字段；
2. 从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X；
3. 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；
4. 从索引 city 取下一个记录的主键 id；
5. 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

“按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。

sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于
sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，**内存放不下，则不得不利用磁盘临时文件辅助排序。**

你可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件。

```sql
/* 打开 optimizer_trace，只对本线程有效 */
SET optimizer_trace='enabled=on'; 
 
/* @a 保存 Innodb_rows_read 的初始值 */
select VARIABLE_VALUE into @a from  performance_schema.session_status where variable_name = 'Innodb_rows_read';
 
/* 执行语句 */
select city, name,age from t where city='杭州' order by name limit 1000; 
 
/* 查看 OPTIMIZER_TRACE 输出 */
SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G
 
/* @b 保存 Innodb_rows_read 的当前值 */
select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';
 
/* 计算 Innodb_rows_read 差值 */
select @b-@a;
```

这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 number_of_tmp_files 中看到是否使用了临时文件。

![image-20230820165541622](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230820165541622.png)

全排序的 **OPTIMIZER_TRACE** 部分结果

number_of_tmp_files 表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要 12 个文件？内存放不下时，就需要使用外部排序，外部排序一般使用
**归并排序算法**。可以这么简单理解，**MySQL 将需要排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12
个有序文件再合并成一个有序的大文件。**

如果 sort_buffer_size 超过了需要排序的数据量的大小，**number_of_tmp_files** 就是 0，表示排序可以直接在内存中完成。

否则就需要放在临时文件中排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大。

接下来，我再和你解释一下图 4 中其他两个值的意思。

我们的示例表中有 4000 条满足 city='/杭州’的记录，所以你可以看到 examined_rows=4000，表示参与排序的行数是 4000 行。

sort_mode 里面的 packed_additional_fields 的意思是，排序过程对字符串做了“紧凑”处理。即使 name 字段的定义是 varchar(16)
，在排序过程中还是要按照实际长度来分配空间的。

同时，最后一个查询语句 select @b-@a 的返回结果是 4000，表示整个执行过程只扫描了 4000 行。

这里需要注意的是，为了避免对结论造成干扰，我把 internal_tmp_disk_storage_engine 设置成 MyISAM。否则，select @b-@a 的结果会显示为
4001。

这是因为查询 OPTIMIZER_TRACE 这个表时，需要用到临时表，而 internal_tmp_disk_storage_engine 的默认值是 InnoDB。如果使用的是
InnoDB 引擎的话，把数据从临时表取出来的时候，会让 Innodb_rows_read 的值加 1。

## RowId 排序

**sort_buffer只存放索引列和主键Id当在sort_buffer或者借助临时文件排序完成后在又一次回表操作之后才能返回结果集。**

在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么
sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。

所以如果单行很大，这个方法效率不够好。

那么，**如果 MySQL 认为排序的单行长度太大会怎么做呢？**

接下来，我来修改一个参数，让 MySQL 采用另外一种算法。

```sql
#默认 4096
SET max_length_for_sort_data = 16; 
```

**max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL
就认为单行太大，要换一个算法。**

city、name、age 这三个字段的定义总长度是 36，我把 max_length_for_sort_data 设置为 16，我们再来看看计算过程有什么改变。

新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。

但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：

1. 初始化 sort_buffer，确定放入两个字段，即 name 和 id；
2. 从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X；
3. 到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；
4. 从索引 city 取下一个记录的主键 id；
5. 重复步骤 3、4 直到不满足 city='杭州’条件为止，也就是图中的 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 进行排序；
7. 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。

这个执行流程的示意图如下，我把它称为 rowid 排序。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/dc92b67721171206a302eb679c83e86d.jpg)

图 5 rowid 排序

对比图 3 的全字段排序流程图你会发现，rowid 排序多访问了一次表 t 的主键索引，就是步骤 7。

需要说明的是，最后的“结果集”是一个逻辑概念，实际上 MySQL 服务端从排序后的 sort_buffer 中依次取出 id，然后到原表查到
city、name 和 age 这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。

根据这个说明过程和图示，你可以想一下，这个时候执行 select @b-@a，结果会是多少呢？

现在，我们就来看看结果有什么不同。

首先，图中的 examined_rows 的值还是 4000，表示用于排序的数据是 4000 行。但是 select @b-@a 这个语句的值变成 5000 了。

因为这时候除了排序过程外，在排序完成后，还要根据 id 去原表取值。由于语句是 limit 1000，因此会多读 1000 行。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/27f164804d1a4689718291be5d10f89b.png)

图 6 rowid 排序的 OPTIMIZER_TRACE 部分输出

从 OPTIMIZER_TRACE 的结果中，你还能看到另外两个信息也变了。

- sort_mode 变成了 <sort_key, rowid>，表示参与排序的只有 name 和 id 这两个字段。
- number_of_tmp_files 变成 10 了，是因为这时候参与排序的行数虽然仍然是 4000 行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了。

## 全字段排序 VS rowid 排序

我们来分析一下，从这两个执行流程里，还能得出什么结论。

如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。

如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。

这也就体现了 MySQL 的一个设计思想：**如果内存够，就要多利用内存，尽量减少磁盘访问。**

对于 InnoDB 表来说，**rowid 排序会要求回表多造成磁盘读**，因此不会被优先选择。

这个结论看上去有点废话的感觉，但是你要记住它，下一篇文章我们就会用到。

看到这里，你就了解了，MySQL 做排序是一个成本比较高的操作。那么你会问，是不是所有的 order by
都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。

其实，并不是所有的 order by 语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，
**其原因是原来的数据都是无序的。**

你可以设想下，如果能够保证从 city 这个索引上取出来的行，天然就是按照 name 递增排序的话，是不是就可以不用再排序了呢？

确实是这样的。

所以，我们可以在这个市民表上创建一个 city 和 name 的联合索引，对应的 SQL 语句是：

```sql
alter table t add index city_user(city, name);
```

作为与 city 索引的对比，我们来看看这个索引的示意图。

图 7 city 和 name 联合索引示意图

在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足 city='杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要
city 的值是杭州，name 的值就一定是有序的。

这样整个查询过程的流程就变成了：

1. 从索引 (city,name) 找到第一个满足 city='杭州’条件的主键 id；
2. 到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回；
3. 从索引 (city,name) 取下一个记录主键 id；
4. 重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/3f590c3a14f9236f2d8e1e2cb9686692.jpg)

图 8 引入 (city,name) 联合索引后，查询语句的执行计划

可以看到，这个查询过程不需要临时表，也不需要排序。接下来，我们用 explain 的结果来印证一下。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/fc53de303811ba3c46d344595743358a.png)

图 9 引入 (city,name) 联合索引后，查询语句的执行计划

从图中可以看到，Extra 字段中没有 Using filesort 了，也就是不需要排序了。而且由于 (city,name) 这个联合索引本身有序，所以这个查询也不用把
4000 行全都读一遍，只要找到满足条件的前 1000 条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描 1000 次。

既然说到这里了，我们再往前讨论，**这个语句的执行流程有没有可能进一步简化呢？**不知道你还记不记得，我在第 5
篇文章[《 深入浅出索引（下）》]中，和你介绍的覆盖索引。

这里我们可以再稍微复习一下。**覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。**

按照覆盖索引的概念，我们可以再优化一下这个查询语句的执行流程。

针对这个查询，我们可以创建一个 city、name 和 age 的联合索引，对应的 SQL 语句就是：

```sql
alter table t add index city_user_age(city, name, age);
```

这时，对于 city 字段的值相同的行来说，还是按照 name 字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了：

1. 从索引 (city,name,age) 找到第一个满足 city='杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回；
2. 从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；
3. 重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/df4b8e445a59c53df1f2e0f115f02cd6.jpg)

图 10 引入 (city,name,age) 联合索引后，查询语句的执行流程

然后，我们再来看看 explain 的结果。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/9e40b7b8f0e3f81126a9171cc22e3423.png)

图 11 引入 (city,name,age) 联合索引后，查询语句的执行计划

可以看到，Extra 字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多。

当然，这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。

**in和exsits优化**

原则：**小表驱动大表**，即小的数据集驱动大的数据集

**in：**当B表的数据集小于A表的数据集时，in优于exists

**exists：**当A表的数据集小于B表的数据集时，exists优于in。**（将主查询的数据放到子查询中做条件验证，根据验证结果(true or
false)来决定主查询的数据结果是否得以保留。）**

一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的，这个是要区分环境的。

- 如果查询的两个表大小相当，那么用in和exists差别不大。
- 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。
- not in 和not exists：如果查询语句使用了not in，那么内外表都进行全表扫描，没有用到索引；而not
  extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。

**count()查询优化**

**四个sql的执行计划一样，说明这四个sql执行效率应该差不多**

**字段有索引：count(\*)≈count(1)>count(字段)>count(主键 id)    //字段有索引，count(字段)
统计走二级索引，二级索引存储数据比主键索引少，所以count(字段)>count(主键 id)*

**字段无索引：count(\*)≈count(1)>count(主键 id)>count(字段)    //字段没有索引count(字段)统计走不了索引，count(主键 id)
还可以走主键索引，所以count(主键 id)>count(字段)**

count(1)跟count(字段)执行过程类似，不过count(1)不需要取出字段统计，就用常量1做统计，count(字段)
还需要取出字段，所以理论上count(1)比count(字段)会快一点。

count(**) 是例外，mysql并不会把全部字段取出来，而是专门做了优化，不取值，按行累加，效率很高，所以不需要用count(列名)或count(
常量)来替代 count(*)。

为什么对于count(id)
，mysql最终选择辅助索引而不是主键聚集索引？因为二级索引相对主键索引存储数据更少，检索性能应该更高，mysql内部做了点优化(
应该是在5.7版本才优化)。

# 事务

事务是由一组SQL语句组成的逻辑处理单元,事务具有以下4个属性,通常简称为事务的ACID属性。

- **原子性(Atomicity)** ：事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行。

- **一致性(Consistent)** ：在事务开始和完成时,数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改,以保持数据的完整性。

- **隔离性(Isolation)** ：数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的,
  反之亦然，多个事务并发执行时，一个事务的执行不应影响其他事务的执行。

- **持久性(Durable)** ：事务完成之后,它对于数据的修改是永久性的,即使出现系统故障也能够保持。

并发事务处理带来的问题

##### 更新丢失（Lost update）或脏写

 当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题–*
*最后的更新覆盖了由其他事务所做的更新**。

##### 脏读（Dirty Reads）


一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致的状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此作进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫做“脏读”。

一句话：**事务A读取到了事务B已经修改但尚未提交的数据**，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合一致性要求。

##### 不可重读（Non-Repeatable Reads）

一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。

一句话：**事务A内部的相同查询语句在不同时刻读出的结果不一致，不符合隔离性**

##### 幻读（Phantom Reads）

**快照视图读取的数据不足以支撑接下来的业务操作。**快照读视图里面发现没有要插入的数据，然后进行insert的时候发现提示数据已存在，这种情况下就叫幻读。
**明明按照第一次快照视图里面没有该数据，自己插入确失败了，以为自己看错了刚才的视图数据，这就是所谓的幻读。**

**当同一个查询在不同的时间产生不同的行集时，事务中就会出现所谓的幻影问题。**例如，如果SELECT执行了两次，但第二次返回的行不是第一次返回的，那么该行就是“幻影”行。

**参考文章：**https://segmentfault.com/a/1190000016566788

一句话就是事务A添加以为没有的数据其实事务B已经添加完成了，所以出现了错误。

![https://note.youdao.com/yws/public/resource/354ae85f3519bac0581919a458278a59/xmlnote/74624CB778F948349A31BA0A40430F51/98786](https://note.youdao.com/yws/public/resource/354ae85f3519bac0581919a458278a59/xmlnote/74624CB778F948349A31BA0A40430F51/98786)

**不可重复读和幻读区别：**

**不可重复读的重点是两次查询结果不同，幻读的重点在于新增或者删除**

**但是会有幻读的问题，幻读修改或添加以为在视图中没有或可修改的数据失败了**

数据库的事务隔离越严格,并发副作用越小,但付出的代价也就越大,因为事务隔离实质上就是使事务在一定程度上“串行化”进行,这显然与“并发”是矛盾的。

同时,不同的应用对读一致性和事务隔离程度的要求也是不同的,比如许多应用对“不可重复读"和“幻读”并不敏感,可能更关心数据并发访问的能力。

## **常看当前数据库的事务隔离级别**

**show variables like 'tx_isolation';**

**设置事务隔离级别：**set tx_isolation='REPEATABLE-READ';

**Mysql默认的事务隔离级别是可重复读，用Spring开发程序时，如果不设置隔离级别默认用Mysql设置的隔离级别，如果Spring设置了就用已经设置的隔离级别
**

##### **锁详解**

锁是计算机协调多个进程货线程并发访问某一资源的机制。

##### *锁分类*

- 从性能上分为乐观锁(用版本对比来实现)和悲观锁

- 从对数据库操作的类型分，分为读锁和写锁(都属于悲观锁)

  读锁（共享锁，S锁(**S**hared)）：针对同一份数据，多个读操作可以同时进行而不会互相影响

  写锁（排它锁，X锁(Exclusive)）：当前写操作没有完成前，它会阻断其他写锁和读锁

- 从对数据操作的粒度分，分为表锁和行锁

**表锁**

每次操作锁住整张表。开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低；一般用在整表数据迁移的场景。

lock table 表名称 read（write），表名称2 read（write）

show open tables；

unlock tables
**案例分析（加读锁）**

lock table mylock read

当前session和其他session都可以读该表

当前session中插入或者更新锁定的表都会报错，其他session插入或更新则会等待

**案例分析(加写锁）**

lock table mylock write；

当前session对该表的增删改查都没有问题，其他session对该表的所有操作被阻塞

## MVCC多版本并发控制机制

**MVCC 是一种并发控制机制，用于在多个并发事务同时读写数据库时保持数据的一致性和隔离性。它是通过在每个数据行上维护多个版本的数据来实现的。
**

**Mysql在可重复读隔离级别下如何保证事务较高的隔离性，同样的sql查询语句在一个事务里多次执行查询结果相同，就算其它事务对数据有修改也不会影响当前事务sql语句的查询结果。
**

**为了防止数据库中的版本无限增长，MVCC 会定期进行版本的回收。回收机制会删除已经不再需要的旧版本数据，从而释放空间。purge线程
**

这个隔离性就是靠MVCC(**Multi-Version Concurrency Control**)机制来保证的，对一行数据的读和写两个操作默认是不会通过加锁互斥来保证隔离性，避免了频繁加锁互斥，
**而在串行化隔离级别为了保证较高的隔离性是通过将所有操作加锁互斥来实现的。该级别通过强制事务按序执行，使不同事务之间不可能产生冲突。
**

InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据
row trx_id 和一致性视图确定数据版本的可见性。

[幻读（Phantom Reads）](#幻读（Phantom Reads）)

**Mysql在读已提交和可重复读隔离级别下都实现了MVCC机制。**

在 `InnoDB` 存储引擎中，创建一个新事务后，执行每个 `select` 语句前，都会创建一个快照（Read View），**快照中保存了当前数据库系统中正处于活跃（没有
commit）的事务的 ID 号**。其实简单的说保存的是系统中当前不应该被本事务看到的其他事务 ID 列表（即 m_ids）。*
*当用户在这个事务中要读取某个记录行的时候**，`InnoDB` 会将该记录行的 `DB_TRX_ID` 与 `Read View` 中的一些变量及当前事务 ID
进行比较，判断是否满足可见性条件。

**Read View生成的时间**

第一种begin /start transation启动方式，一致性视图是在执行第一个快照读语句时创建的；

第二种启动方式，一致性视图是在执行start transaction with consistent snapshot 时创建的。

**begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用
start transaction with consistent snapshot 这个命令。**

**undo日志版本链与read view机制详解**

undo日志版本链是指一行数据被多个事务依次修改过后，在每个事务修改完后，Mysql会保留修改前的数据undo回滚日志，并且用两个隐藏字段trx_id和roll_pointer把这些undo日志串联起来形成一个历史记录版本链(
见下图，需参考视频里的例子理解)

![0](https://note.youdao.com/yws/public/resource/b36b975188fadf7bfbfd75c0d2d6b834/xmlnote/8C9D98D4BB9B49BEB2822BCE98A37DDE/99285)

在**可重复读隔离级别**，当事务开启，执行任何（select）查询sql时会生成当前事务的*
*一致性视图read-view，**该视图在事务结束之前都不会变化(**如果是读已提交隔离级别在每次执行查询sql时都会重新生成**)
，这个视图由执行查询时所有**未提交事务id数组**（**数组里最小的事务id为min_id**）和**已创建的最大事务id（max_id）**
组成，事务里的任何sql查询结果需要从对应版本链里的最新数据开始逐条跟read-view做比对从而得到最终的快照结果。

**版本链比对规则：**

1. 如果 row 的 trx_id 落在绿色部分( trx_id<min_id),表示这个版本是已提交的事务生成的，这个数据是可见的。

2. 如果 row 的 trx_id 落在红色部分( trx_id>max_id )，表示这个版本是由将来启动的事务生成的，是不可见的(若 row 的 trx_id
   就是当前自己的事务是可见的）；

3. 如果 row 的 trx_id 落在黄色部分(min_id <=trx_id<= max_id)，那就包括两种情况

a. 若 row 的 trx_id 在视图数组中，表示这个版本是由还没提交的事务生成的，不可见(若 row 的 trx_id
就是当前自己的事务是可见的)；

b. 若 row 的 trx_id 不在视图数组中，表示这个版本是已经提交了的事务生成的，可见。

对于删除的情况可以认为是update的特殊情况，会将版本链上最新的数据复制一份，然后将trx_id修改成删除操作的trx_id，同时在该条记录的头信息（record
header）里的（deleted_flag）标记位写上true，来表示当前记录已经被删除，在查询时按照上面的规则查到对应的记录如果delete_flag标记位为true，
**意味着记录已被删除，则不返回数据。**

**注意：**begin/start transaction
命令并不是一个事务的起点，在执行到它们之后的第一个修改操作InnoDB表的语句，事务才真正启动，才会向mysql申请事务id，mysql内部是严格按照事务的启动顺序来分配事务id的。

**事务查询查询之后再更新数据就不能是在历史版本上更新了，更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current
read）。而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。**

**而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：**

- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

**总结：**

MVCC机制的实现就是通过read-view机制与undo版本链比对机制，使得不同的事务会根据数据版本链对比规则读取同一条数据在版本链上的不同版本数据。

**可重复读的情况下还是会发生幻读问题。**

![img](https://img-blog.csdnimg.cn/img_convert/d21f7c93d96874d62085ee234ade116e.png)

在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id =
5的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新纪录的 trx_id 隐藏列的值就变成了事务 A 的事务
id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。

因为这种特殊场景的存在，在首次查询之后，进行了修改操作使用了当前读，然后再进行查询，得到了当前读的内容，**所以我们认为 MySQL
Innodb 中的 MVCC 并不能完全避免幻读现象**。

解决方法就是一开始上来就使用当前读 加上for update 就会上锁。next key log （前开后闭）有可能退变成为行锁和间隙锁（前开后开）

### 行锁(Record Lock)

锁定单个行记录上的锁，如果没有设置任何索引会使用隐式的主键进行锁定。

### 间隙锁(Gap Lock)

锁定一个范围，但不包括记录本身，是为了阻止多个事务将记录插入到同一个范围内。**前开后开区间**

**关闭Gap Lock两种方式：**

1、将事务的隔离级别设置为**READ COMMITTED**

2、将参数**log_statements_unsafe_for_binlog**设置为off 默认开启

### 临键锁(Next-key Locks)

Gap Lock+Record Lock，锁定一个范围，并且锁定记录本身，当查询的索引含有**唯一属性**时，会对next-key lock进行优化，将其降级为Record
Lock锁。不能是联合索引，如果是还是会用next-key lock锁定。

**当查询的索引是辅助索引时，记录本身既拥有聚集索引也拥有辅助索引，Next-key lock会对主键索引加上Record
lock锁，然后对于辅助索引加上前闭后闭Gap Lock，并同时还会对辅助索引下一个键值对加上gap lock**

数组的前闭后开、前闭后闭，**[ , ]中括号表示能取得这个数,称为“闭” ( , )小括号表示不能取到这个数，称为“开”。**

InnoDB存储引擎默认的事务隔离级别是Repeatable Read，在该隔离级别下，其采用Next-Key Locking的方式来加锁。而在事务隔离级别Read
Committed下，其仅采用Record Lock。

## 加锁规则

行级锁加锁规则比较复杂，不同的场景，加锁的形式是不同的。

**加锁的对象是索引，加锁的基本单位是 next-key lock**，它是由记录锁和间隙锁组合而成的，**next-key lock 是前开后闭区间，而间隙锁是前开后开区间
**。但是，next-key lock 在一些场景下会退化成记录锁或间隙锁。

### 唯一索引等值查询

当我们用唯一索引进行等值查询的时候，查询的记录存不存在，加锁的规则也会不同：

- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会**退化成「记录锁」**。
- 当查询的记录是「不存在」的，则会在索引树找到**第一条大于该查询记录的记录**，然后将该记录的索引中的 next-key lock 会*
  *退化成「间隙锁」**

### 唯一索引范围查询

范围查询和等值查询的加锁规则是不同的。

当唯一索引进行范围查询时，会对每一个扫描到的索引加 next-key 锁，然后如果遇到下面这些情况，会退化成记录锁或者间隙锁：

- 情况一：针对「大于等于」的范围查询，因为存在等值查询的条件，那么如果等值查询的记录是存在于表中，那么该记录的索引中的
  next-key 锁会**退化成记录锁**。
- 情况二：针对「小于或者小于等于」的范围查询，要看条件值的记录是否存在于表中：
    1. **当条件值的记录不在表中**，那么不管是「小于」还是「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引的
       next-key 锁会**退化成间隙锁**，其他扫描到的记录，都是在这些记录的索引上加 **next-key 锁**。
    2. **当条件值的记录在表中**，如果是**「小于」**条件的范围查询，扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会**
       退化成间隙锁**，其他扫描到的记录，都是在这些记录的索引上加**next-key 锁**。
    3. **当条件值的记录在表中**，如果**「小于等于**」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引 next-key 锁*
       *不会退化成间隙锁**。其他扫描到的记录，都是在这些记录的索引上加 **next-key 锁**。

### 非唯一索引等值查询

当我们用非唯一索引进行等值查询的时候，*
*因为存在两个索引，一个是主键索引，一个是非唯一索引（二级索引），所以在加锁时，同时会对这两个索引都加锁，但是对主键索引加锁的时候，只有满足查询条件的记录才会对它们的主键索引加锁
**。

针对非唯一索引等值查询时，查询的记录存不存在，加锁的规则也会不同：

- 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是*
  *非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是
  next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁
  **。
- 当查询的记录「不存在」时，**扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key
  锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁**。

### 非唯一索引范围查询

非唯一索引和主键索引的范围查询的加锁也有所不同，不同之处在于**非唯一索引范围查询，索引的 next-key lock 不会有退化为间隙锁和记录锁的情况
**，也就是非唯一索引进行范围查询时，对二级索引记录加锁都是加 next-key 锁。

因此在使用过程中尽可能使用主键索引和唯一索引进行处理，避免大面积的锁定造成性能的一个影响。

### 没有加索引的查询

**如果锁定读查询语句，没有使用索引列作为查询条件，或者查询语句没有走索引查询，导致扫描是全表扫描。那么，每一条记录的索引上都会加
next-key 锁，这样就相当于锁住的全表，这时如果其他事务对该表进行增、删、改操作的时候，都会被阻塞**。

不只是锁定读查询语句不加索引才会导致这种情况，update 和 delete 语句如果查询条件不加索引，那么由于扫描的方式是全表扫描，于是就会对每一条记录的索引上都会加
next-key 锁，这样就相当于锁住的全表。

**参考文章：**https://www.toutiao.com/article/7240633218465088037/?source=aweme_search

# sql执行流程

![0](https://note.youdao.com/yws/public/resource/b36b975188fadf7bfbfd75c0d2d6b834/xmlnote/9C296B9BBF3C4C0389F470357FC55FE9/99001)

**为什么Mysql不能直接更新磁盘上的数据而且设置这么一套复杂的机制来执行SQL了？**

**因为来一个请求就直接对磁盘文件进行随机读写，然后更新磁盘文件里的数据性能可能相当差。因为磁盘随机读写的性能是非常差的，所以直接更新磁盘文件是不能让数据库抗住很高并发的。
**Mysql这套机制看起来复杂，但它可以保证每个更新请求都是**更新内存BufferPool**，然后**顺序写日志文件**
，同时还能保证各种异常情况下的数据一性。更新内存的性能是极高的，然后顺序写磁盘上的日志文件的性能也是非常高的，要远高于随机读写磁盘文件。正是通过这套机制，才能让我们的MySQL数据库在较高配置的机器上每秒可以抗下几干甚至上万的读写请求。

**参考文章**：https://javaguide.cn/database/mysql/innodb-implementation-of-mvcc.html#readview

**简单来说redolog是两阶段提交，先写入redolog此时是prepare状态，然后告诉执行器执行完成了，并把binglog写入磁盘，最后引擎在把刚刚写入的redo
log改为commit状态，更新完成。**

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2
   这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare
   状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

![image-20230820151008261](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230820151008261.png)

# 两阶段提交

为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起：*
*怎样让数据库恢复到半个月内任意一秒的状态？**

前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。**如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有
binlog**，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

- 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
- 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。

由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写
binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。

仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了
crash，会出现什么情况呢？

1. **先写 redo log 后写 binlog**。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。**由于我们前面说过的，redo
   log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1**。 但是由于 binlog 没写完就 crash 了，这时候
   binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 然后你会发现，如果需要用这个
   binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是
   0，与原库的值不同。
2. **先写 binlog 后写 redo log**。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c
   的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行
   c 的值就是 1，与原库的值不同。

可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。

你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？

其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用
binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。

**简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。**

redo log 用于保证 crash-safe 能力。**innodb_flush_log_at_trx_commit** 这个参数设置成 1 的时候，表示每次事务的 redo log
都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

**sync_binlog** 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证
MySQL 异常重启之后 binlog 不丢失。

## 日志文件

### undo log

回滚行记录到某个特定版本，**undo
log时逻辑日志，因此只是将数据库逻辑地恢复到原来的样子。所有修改都被逻辑的取消了，但是数据结构和页本身在回滚之后可能大不相同。由于并发事务，所以不能将一个页回滚到事务开始的样子。
**

**注意：undo log会产生redo log，也就是undo log的产生会随着redo log的产生，这是因为undo log也需要持久性的保护。**

在InnoDB存储引擎中，undo log分为：

insert undo log

update undo log

insert undo log 指的是在insert操作中产生的undo log。对事务本身可见，对其他事务不可见。故该undo
log可以在事务提交后直接删除。不需要进行purge操作。

。我们前面介绍`InnoDB`的数据字典时说过，每个表都会被分配一个唯一的`table id`，我们可以通过系统数据库`information_schema`
中的`innodb_sys_tables`表来查看某个表对应的`table id`是什么。

```sql
mysql> SELECT * FROM information_schema.INNODB_TABLES WHERE name = 't1';
```

![image-20230821183957823](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230821183957823.png)

一条日志有不同的类型 记录的内容也不尽相同

有对应insert的delete update的

对于insert来说只需要记录对应的插入主键id就可以了 删除的时候时候，对于插入到索引页的数据来说，伴随着每行记录会指向插入到undo页的地址，每行都有隐藏指针roll_pointer
7字节 事务6字节 隐藏id6字节。

**INSERT操作对应的undo日志**

![image-20230821194458700](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230821194458700.png)

![image-20230821194511968](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230821194511968.png)

**DELETE操作对应的undo日志**

我们知道插入到页面中的记录会根据记录头信息中的`next_record`属性组成一个单向链表，我们把这个链表称之为`正常记录链表`
；我们在前面介绍数据页结构的时候说过，被删除的记录其实也会根据记录头信息中的`next_record`
属性组成一个链表，只不过这个链表中的记录占用的存储空间可以被重新利用，所以也称这个链表为`垃圾链表`。`Page Header`
部分有一个称之为`PAGE_FREE`的属性，它指向由被删除记录组成的垃圾链表中的头节点。

![img](https://relph1119.github.io/mysql-learning-notes/images/22-06.png)

从图中可以看出，`正常记录链表`中包含了3条正常记录，`垃圾链表`里包含了2条已删除记录，在`垃圾链表`
中的这些记录占用的存储空间可以被重新利用。页面的`Page Header`部分的`PAGE_FREE`属性的值代表指向`垃圾链表`
头节点的指针。假设现在我们准备使用`DELETE`语句把`正常记录链表`中的最后一条记录给删除掉，其实这个删除的过程需要经历两个阶段：

- 阶段一：仅仅将记录的`delete_mask`标识位设置为`1`，其他的不做修改（其实会修改记录的`trx_id`、`roll_pointer`
  这些隐藏列的值）。设计`InnoDB`的大佬把这个阶段称之为`delete mark`。

![img](https://relph1119.github.io/mysql-learning-notes/images/22-07.png)

可以看到，`正常记录链表`中的最后一条记录的`delete_mask`值被设置为`1`，但是并没有被加入到`垃圾链表`
。也就是此时记录处于一个`中间状态`
，跟猪八戒照镜子——里外不是人似的。在删除语句所在的事务提交之前，被删除的记录一直都处于这种所谓的`中间状态`。

```txt
小贴士：为什么会有这种奇怪的中间状态呢？其实主要是为了实现一个称之为MVCC的功能，稍后再介绍。
```

阶段二：当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉。所谓真正的删除就是把该记录从`正常记录链表`
中移除，并且加入到`垃圾链表`中，然后还要调整一些页面的其他信息，比如页面中的用户记录数量`PAGE_N_RECS`
、上次插入记录的位置`PAGE_LAST_INSERT`、垃圾链表头节点的指针`PAGE_FREE`、页面中可重用的字节数量`PAGE_GARBAGE`
、还有页目录的一些信息等等。设计`InnoDB`的大佬把这个阶段称之为`purge`。

把`阶段二`执行完了，这条记录就算是真正的被删除掉了。这条已删除记录占用的存储空间也可以被重新利用了。画下来就是这样：

![img](https://relph1119.github.io/mysql-learning-notes/images/22-08.png)

> 页面的Page Header部分有一个**PAGE_GARBAGE**
> 属性，该属性记录着当前页面中可重用存储空间占用的总字节数。每当有已删除记录被加入到垃圾链表后，都会把这个PAGE_GARBAGE属性的值加上该已删除记录占用的存储空间大小。
**PAGE_FREE**
>
指向垃圾链表的头节点，之后每当新插入记录时，首先判断PAGE_FREE指向的头节点代表的已删除记录占用的存储空间是否足够容纳这条新插入的记录，如果不可以容纳，就直接向页面中申请新的空间来存储这条记录（是的，你没看错，并不会尝试遍历整个垃圾链表，找到一个可以容纳新记录的节点）。如果可以容纳，那么直接重用这条已删除记录的存储空间，并且把PAGE_FREE指向垃圾链表中的下一条已删除记录。但是这里有一个问题，如果新插入的那条记录占用的存储空间大小小于垃圾链表的头节点占用的存储空间大小，那就意味头节点对应的记录占用的存储空间里有一部分空间用不到，这部分空间就被称之为碎片空间。那这些碎片空间岂不是永远都用不到了么？其实也不是，这些碎片空间占用的存储空间大小会被统计到PAGE_GARBAGE属性中，这些碎片空间在整个页面快使用完前并不会被重新利用，不过当页面快满时，如果再插入一条记录，此时页面中并不能分配一条完整记录的空间，这时候会首先看一看PAGE_GARBAGE的空间和剩余可利用的空间加起来是不是可以容纳下这条记录，如果可以的话，InnoDB会尝试重新组织页内的记录，重新组织的过程就是先开辟一个临时页面，把页面内的记录依次插入一遍，因为依次插入时并不会产生碎片，之后再把临时页面的内容复制到本页面，这样就可以把那些碎片空间都解放出来（很显然重新组织页面内的记录比较耗费性能）。

![img](https://relph1119.github.io/mysql-learning-notes/images/22-09.png)

UPDATE操作对应的undo日志

在执行`UPDATE`语句时，`InnoDB`对更新主键和不更新主键这两种情况有截然不同的处理方案。

**不更新主键**

在不更新主键的情况下，又可以细分为被更新的列占用的**存储空间不发生变化**和**发生变化**的情况。

​        **就地更新（in-place update）**

​    **更新记录时，对于被更新的每个列来说，如果更新后的列和更新前的列占用的存储空间都一样大，那么就可以进行`就地更新`
，也就是直接在原记录的基础上修改对应列的值。**

​        **先删除掉旧记录，再插入新记录**

 在不更新主键的情况下，如果有任何一个被更新的列更新前和更新后占用的存储空间大小不一致，那么就需要先把这条旧的记录从聚簇索引页面中删除掉，然后再根据更新后列的值创建一条新的记录插入到页面中。

**请注意一下，我们这里所说的`删除`并不是`delete mark`操作，而是真正的删除掉，也就是把这条记录从`正常记录链表`
中移除并加入到`垃圾链表`中**，并且修改页面中相应的统计信息（比如`PAGE_FREE`、`PAGE_GARBAGE`
等这些信息）。不过这里做真正删除操作的线程并不是在介绍`DELETE`语句中做`purge`
操作时使用的另外专门的线程，而是由用户线程同步执行真正的删除操作，真正删除之后紧接着就要根据各个列更新后的值创建的新记录插入。

![img](https://relph1119.github.io/mysql-learning-notes/images/22-15.png)

**更新主键的情况**

将旧记录进行`delete mark`操作

根据更新后各列的值创建一条新记录，并将其插入到聚簇索引中（需重新定位插入的位置）。

针对`UPDATE`语句更新记录主键值的这种情况，在对该记录进行`delete mark`操作前，会记录一条类型为`TRX_UNDO_DEL_MARK_REC`
的`undo日志`；之后插入新记录时，会记录一条类型为`TRX_UNDO_INSERT_REC`的`undo日志`
，也就是说每对一条记录的主键值做改动时，会记录2条`undo日志`。

在一个事务执行过程中，可能混着执行`INSERT`、`DELETE`、`UPDATE`语句，也就意味着会产生不同类型的`undo日志`
。但是我们前面又强调过，同一个`Undo页面`要么只存储`TRX_UNDO_INSERT`大类的`undo日志`，要么只存储`TRX_UNDO_UPDATE`
大类的`undo日志`，反正不能混着存，所以在一个事务执行过程中就可能需要2个`Undo页面`的链表，一个称之为`insert undo链表`
，另一个称之为`update undo链表`，画个示意图就是这样：

![img](https://relph1119.github.io/mysql-learning-notes/images/23-08.png)

另外，设计`InnoDB`的大佬规定对普通表和临时表的记录改动时产生的`undo日志`
要分别记录（我们稍后阐释为什么这么做），所以在一个事务中最多有4个以`Undo页面`为节点组成的链表：

![img](https://relph1119.github.io/mysql-learning-notes/images/23-09.png)

![img](https://relph1119.github.io/mysql-learning-notes/images/23-22.png)

在修改针对普通表的回滚段中的Undo页面时，需要记录对应的redo日志，而修改针对临时表的回滚段中的Undo页面时，不需要记录对应的redo日志。

**如果没有可分配更多的undo页面链表**

```txt
Too many active concurrent transactions
活动并发事务过多
```

**Undo页面链表**

**单个事务中的Undo页面链表**

因为一个事务可能包含多个语句，而且一个语句可能对若干条记录进行改动，而对每条记录进行改动前，都需要记录1条或2条的`undo日志`
，所以在一个事务执行过程中可能产生很多`undo日志`
，这些日志可能一个页面放不下，需要放到多个页面中，这些页面就通过我们上面介绍的`TRX_UNDO_PAGE_NODE`属性连成了链表。

在一个事务执行过程中，可能混着执行`INSERT`、`DELETE`、`UPDATE`语句，也就意味着会产生不同类型的`undo日志`
。但是我们前面又强调过，同一个`Undo页面`要么只存储`TRX_UNDO_INSERT`大类的`undo日志`，要么只存储`TRX_UNDO_UPDATE`
大类的`undo日志`，反正不能混着存，所以在一个事务执行过程中就可能需要2个`Undo页面`的链表，一个称之为`insert undo链表`
，另一个称之为`update undo链表`。

**另外，设计`InnoDB`的大佬规定对普通表和临时表的记录改动时产生的`undo日志`
要分别记录（我们稍后阐释为什么这么做），所以在一个事务中最多有4个以`Undo页面`为节点组成的链表。按需分配，什么时候需要什么时候再分配，不需要就不分配。
**

**多个事务中的Undo页面链表**

为了尽可能提高`undo日志`的写入效率，不同事务执行过程中产生的undo日志需要被写入到不同的Undo页面链表中。

**Undo log segment**

每一个`Undo页面`链表都对应着一个`段`，称之为`Undo Log Segment`
。也就是说链表中的页面都是从这个段里边申请的，所以他们在`Undo页面`链表的第一个页面，也就是上面提到的`first undo page`
中设计了一个称之为`Undo Log Segment Header`的部分，这个部分中包含了该链表对应的段的`segment header`
信息以及其他的一些关于这个段的信息，所以`Undo`页面链表的第一个页面其实长这样：

![image-20230821205727283](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230821205727283.png)

`TRX_UNDO_STATE`：本`Undo页面`链表处在什么状态。

一个`Undo Log Segment`可能处在的状态包括：

- `TRX_UNDO_ACTIVE`：活跃状态，也就是一个活跃的事务正在往这个段里边写入`undo日志`。
- `TRX_UNDO_CACHED`：被缓存的状态。处在该状态的`Undo页面`链表等待着之后被其他事务重用。
- `TRX_UNDO_TO_FREE`：对于`insert undo`链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。
- `TRX_UNDO_TO_purge`：对于`update undo`链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。
- `TRX_UNDO_PREPARED`：包含处于`PREPARE`阶段的事务产生的`undo日志`。

- `TRX_UNDO_LAST_LOG`：本`Undo页面`链表中最后一个`Undo Log Header`的位置。
- `TRX_UNDO_FSEG_HEADER`：本`Undo页面`链表对应的段的`Segment Header`
  信息（就是我们上一节介绍的那个10字节结构，通过这个信息可以找到该段对应的`INODE Entry`）。
- `TRX_UNDO_PAGE_LIST`：`Undo页面`链表的基节点。

**重用Undo页面**

一个`Undo页面`链表是否可以被重用的条件很简单：

- 该链表中只包含一个`Undo页面`。
- 该`Undo页面`已经使用的空间小于整个页面空间的3/4。

**insert undo链表**

提交后记录没用了，直接覆盖

**update undo链表**

继续添加undo log

**回滚段**

**Rollback Segment下面可以管理多个undo log segment**

一个Rollback segment只有1024个undo slot 也就是1024个 undo链表，通过参数查看有128个 也就是可以有

128 * 1024 = 131072

```sql
show variables like '%segment%';
```

我们现在知道一个事务在执行过程中最多可以分配4个`Undo页面`链表，在同一时刻不同事务拥有的`Undo页面`
链表是不一样的，所以在同一时刻系统里其实可以有许许多多个`Undo页面`链表存在。为了更好的管理这些链表，设计`InnoDB`
的大佬又设计了一个称之为`Rollback Segment Header`的页面，在这个页面中存放了各个`Undo页面`链表的`frist undo page`的`页号`
，他们把这些`页号`称之为`undo slot`。我们可以这样理解，每个`Undo页面`链表都相当于是一个班，这个链表的`first undo page`
就相当于这个班的班长，找到了这个班的班长，就可以找到班里的其他同学（其他同学相当于`normal undo page`
）。有时候学校需要向这些班级传达一下精神，就需要把班长都召集在会议室，这个`Rollback Segment Header`就相当于是一个会议室。

**purge**

purge用于最终完成delete和update操作。这样的设计是因为InnoDB存储引擎支持MVCC，所以记录不能在事务提交时立即处理。这时其他事务可能正在引用这行，故InnoDB存储引擎需要保存记录之前的版本。而是否可以删除该记录通过purge来进行判断。

InnoDB存储引擎有一个history链表，**它根据事务提交的顺序**，将undo log进行链接。先提交的事务在尾端。因为可以重用，所以多个事务的undolog可以在一个undo
page中。

在执行puge的过程中，InnoDB存储引擎首先从history list中找到第一个需要被清理的记录，这里是trx1，清理之后InnoDB存储引擎会在trx1的undo
log所在的页中继续寻找是否存在可以被清理的记录，这里会找到事务trx3，接着找到trx5，但是发现trx5被其他事物所引用而不能清理，故去再次去history
list中查找，发现这时最尾端的记录为trx2，记者找到trx2所在的页，然后依次再把事务trx6、trx4的记录进行清理。由于undo
page的所有页都被清理了，因此该undo page可以被重用。

InnoDB引擎这种先从history list中找到undo log，然后再从undo page中找undo log的设计模式是为了避免大量的随机读取操作，从而提高purge的效率。

全局动态参数 **innodb_purge_batch_size**用来设置每次purge操作需要清理的undo page数量。

**purge删除的是什么？**

1、把数据页行消息改到垃圾连表中，free garbage。

2，清除undo log

如何判断需不需要删除因为innodb有一个history链表存储的是按照事务提交的顺序将undo log进行链接，先提交的在尾部，然后先根据第一个undo
log然后再看page中其他是否需要清理的undo log。

删除的是update undo log是对 **delete 和 update 操作产生的 undo log**。该 undo log 可能需要提供 MVCC
机制，因此不能在事务提交时就进行删除。提交时放入 undo log 链表，等待 purge 线程进行最后的删除。

事务提交后，InnoDB做2件事：

- 将undo log放入链表中，以供之后的 purge 操作
- 判断 undo log 所在的页是否可以重用，若可以分配给下个事务使用

**事务回滚undolog是如何进行数据恢复的？**

事务回滚是通过Undo Log来进行数据恢复的。Undo Log记录了事务对数据所做的修改操作，包括旧值和新值。

当需要回滚一个事务时，数据库会根据Undo Log中的信息，按照相反的操作进行数据恢复。具体的过程如下：

1. 事务开始：当一个事务开始时，系统会为该事务分配一个唯一的事务ID，并为该事务创建一个Undo段用于存储Undo Log。

2. 记录Undo操作：在事务执行期间，对数据进行修改操作时，系统将修改前的旧值记录到Undo Log中。这样，在回滚操作时可以使用Undo
   Log中的旧值将数据还原到修改之前的状态。

3. 回滚操作：当需要回滚一个事务时，系统会根据Undo
   Log中的信息，按照相反的操作进行数据恢复。例如，对于一个insert操作，回滚操作就是删除插入的数据行；对于一个update操作，回滚操作就是将数据行恢复到修改前的状态。

4. 事务结束：当事务回滚完成后，系统会释放Undo Log所占用的空间。

需要注意的是，Undo Log中的数据只对当前事务可见，其他事务无法访问。这保证了事务的隔离性。

通过使用Undo Log进行事务回滚，数据库可以将数据恢复到事务开始之前的状态，从而保持数据的一致性和完整性。

**innodb的undo log有持久化功能吗？**

**是的，InnoDB引擎的Undo Log具有持久化功能**。Undo Log用于记录事务的回滚操作，以便在事务回滚或崩溃恢复时可以撤销对数据库的更改。

Undo Log的持久化是通过将其写入磁盘上的Undo Log文件来实现的。**当事务进行提交或数据库发生崩溃时，InnoDB会将Undo
Log中的数据刷新到磁盘上的日志文件中，以确保数据的持久性和一致性。**

通过持久化Undo Log，InnoDB可以在数据库故障后进行崩溃恢复。它可以使用Undo Log文件中的信息来撤销未提交的事务所做的更改，并还原已提交的事务所做的更改。

**需要注意的是，Undo Log的持久化是在事务提交之前进行的**，因此如果事务未提交，则对应的Undo Log可能不会持久化到磁盘上。

**innodb的undolog 事务如何和redo log的事务进行关联通过 xid吗？**

InnoDB的Undo Log和Redo Log是用于不同的目的，它们在事务处理中扮演不同的角色。

**Undo Log记录了事务对数据的修改操作，用于事务的回滚。**当事务执行修改操作时，将会生成对应的Undo Log记录，*
*用于在事务回滚或系统崩溃恢复时撤销对数据的修改**。每个事务都有自己的Undo Log，Undo Log中的信息与事务相关联。

**Redo Log记录了事务对数据的修改操作，用于事务的持久化。**当事务执行修改操作时，将会生成对应的Redo
Log记录，用于在系统崩溃后进行重做操作，以确保数据的持久性和一致性。Redo Log中的信息与事务无直接关联，而是以物理日志记录的形式存在。

在InnoDB中，事务的相关信息并不是通过xid（事务ID）来关联Undo Log和Redo Log的，而是通过LSN（Log Sequence
Number）来进行关联。LSN是一个递增的数字，表示日志记录在日志文件中的位置。Undo Log和Redo Log都包含了LSN的信息，通过LSN可以实现两者的关联。

总结来说，Undo Log记录事务的回滚操作，与事务相关联；而Redo Log记录事务的修改操作，与事务没有直接关联，而是通过LSN来关联。这种设计可以提高并发性和恢复性，保证数据的一致性和持久性。

**checkpoint和lsn的关系 数据崩溃恢复的整体流程？**

Checkpoint和LSN（Log Sequence Number）是InnoDB引擎中用于数据崩溃恢复的重要概念。 **
Checkpoint是一个触发点，用于控制数据在内存中和磁盘中的同步。**
当发生Checkpoint时，InnoDB会将内存中被修改的数据页刷新到磁盘上的数据文件中，并将Checkpoint位置的LSN记录下来。这样做的目的是为了减少数据库崩溃后的恢复时间，因为只需要从Checkpoint位置开始进行日志重做即可。
**LSN是一个递增的日志序列号，它标识了日志文件中日志记录的顺序。每个事务的操作都会生成一个对应的LSN**。*
*LSN与数据文件和日志文件中的位置相关联。** 数据崩溃恢复的整体流程如下： **1. 数据库崩溃**
：发生系统故障或非正常停机，导致数据库无法正常工作。 **2. 启动数据库：**
当数据库重新启动时，InnoDB引擎会检查日志文件中的最后一个Checkpoint位置的LSN，并将其标记为恢复起点。 **3. 日志重做**：*
*从恢复起点开始，依次读取日志文件中的日志记录，并根据LSN顺序进行重做操作，将未完成的事务操作应用到数据文件上。这个过程称为日志重做（Redo
Log）**。 **4. 数据恢复：**在日志重做期间，InnoDB会通过Undo Log来撤销未提交的事务操作，以保持数据的一致性。Undo
Log中记录了事务的回滚操作，用于回滚未提交的事务。 **5. 数据一致性：**当所有日志记录都经过重做和回滚操作后，数据库的数据将恢复到崩溃之前的一致状态。
总结来说，**Checkpoint是控制数据同步的触发点**，**LSN用于标识日志记录的顺序**。数据崩溃恢复的流程包括日志重做和回滚操作，以保证数据的一致性和完整性。

**InnoDB的LSN是个什么类型的数据 在redolog每条记录生成的时候回递增吗 之后事务提交才会修改全局的lsn？**

**InnoDB的LSN（日志序列号）是一个64位的无符号整数。**它用于标识InnoDB存储引擎中数据文件和日志文件的位置。 在InnoDB的redo
log（重做日志）中，每条记录生成时都会递增LSN。Redo log是一种事务日志，用于记录正在进行的事务所做的修改操作。每当有新的修改操作发生时，该操作将被记录到redo
log中，并分配一个唯一的LSN。这个LSN值是根据全局的LSN递增产生的，确保了LSN的顺序性。 事务提交时，全局的LSN才会被更新为最后一条redo
log记录的LSN。这样做的目的是为了确保只有在事务成功提交后，全局LSN才会被修改。如果事务在提交之前失败或回滚，那么全局LSN将不会被修改，这有助于数据库恢复时准确地确定需要回滚的操作。
总结起来，InnoDB的LSN是一个64位的无符号整数，用于标识数据文件和日志文件的位置。在redo
log中，每条记录生成时LSN都会递增。事务提交时，全局的LSN才会被更新为最后一条redo log记录的LSN。

**undolog中本事务可见的insert的undolog是哪个线程删除的 purge吗？**

在MySQL中，Undolog是用来记录事务对数据库进行的更改操作的日志。当一个事务提交后，其对应的undolog会被用于回滚或者重做操作。
对于可见的insert操作，当事务被提交后，purge线程会负责删除其对应的undolog。*
*purge线程的主要功能是扫描已经提交的事务，并清理掉它们对应的undolog，以释放存储空间。**
因此，可以说purge线程负责删除可见insert操作的undolog。

**purge删除的都有什么 什么时机删除？**

purge线程主要负责**删除MySQL中已经提交的事务对应的undolog**，**以释放存储空间**。以下是purge线程删除的一些内容：

1. 已经提交的事务：当一个事务提交后，purge线程会扫描该事务对应的undolog，并将其删除。
2. 超过保留期限的undolog：在MySQL中，有一个参数叫做`innodb_purge_rseg_truncate_frequency`
   ，用于控制purge线程的运行频率。当purge线程运行时，它会检查已经提交的事务的保留期限，超过保留期限的undolog会被删除。
   频率删除undolog，
3. 空闲页：当一个事务删除了某些记录，而这些记录所在的页变得空闲时，purge线程会负责删除这些空闲页。
   关于purge线程的具体时间删除操作，取决于多个因素，例如系统负载、数据库配置和活动事务数量等。通常情况下，purge线程会在数据库空闲或者系统负载较低时运行，以免影响正常的数据库操作。

**purge是如何删除对应的undolog的 通过什么实现？**

purge线程通过以下方式来删除对应的undolog：

1. 通过读取系统表空间中的**数据字典来确定哪些事务已经提交**。
2. 根据提交的事务信息，定位到对应的undolog文件和偏移量。
3. 使用预定义的算法和数据结构，遍历undolog文件，标记并删除已经提交的事务的日志记录。
4. **在遍历过程中，如果发现某个页上的所有事务已经提交，并且该页没有被其他未提交的事务所引用，purge线程会将该页回收并释放空间。
   **
   具体而言，MySQL的purge线程使用了类似于多版本并发控制（MVCC）的机制来处理事务的回滚和重做操作。每个事务都有一个唯一的ID，当一个事务提交后，purge线程会根据事务ID进行判断和处理。通过不断地扫描undolog文件，purge线程可以快速地识别并删除已经提交的事务的undolog，从而释放存储空间。
   需要注意的是，purge线程的运行方式和实现细节可能因不同的数据库引擎而有所差异。上述描述主要适用于InnoDB存储引擎，在其他存储引擎中可能存在一些差异。

**参数 innodb_purge_rseg_truncate_frequency的具体作用？**

参数 `innodb_purge_rseg_truncate_frequency` 是用于控制InnoDB存储引擎中purge线程的运行频率的一个参数。它的作用是控制purge线程在扫描已提交事务的undolog并删除过期记录时的频率。
具体而言，**该参数定义了purge线程在一次扫描过程中所处理的重做段（redo segment）数量**。重做段是undolog的逻辑单位，每个重做段包含多个事务的日志记录。
该参数的默认值为`128`，**意味着每次purge线程运行时，会处理128个重做段。当purge线程完成一次扫描后，会休眠一段时间，然后再次运行。
** 调整`innodb_purge_rseg_truncate_frequency`参数的值可以影响purge线程的运行频率和对系统资源的占用情况。*
*较小的值会使purge线程更频繁地运行**，可能会增加系统负载；而**较大的值则会减少purge线程的运行频率**，可能会导致undolog占用的存储空间增加。
需要根据实际情况进行调整，以平衡数据库**性能和存储空间的需求**。一般来说，如果系统中有**大量短暂的事务或者频繁的写入操作，较小的值可能更适合
**；而如果系统中有**较少的长期事务或者较少的写入操作，较大的值可能更合适。**

**InnoDB redolog的组提交的过程，跟lsn有什么关系，具体执行流程，以及binlog的组提交的完整过程？**

InnoDB的Redo Log和LSN在组提交过程中起着重要的作用，而Binlog的组提交过程则与之略有不同。下面是它们各自的执行流程：

**InnoDB Redo Log组提交过程：**

1. 在事务提交前，所有的修改操作会被写入Redo Log，每个修改操作都会被赋予一个唯一的LSN值。
2. 当一个事务提交时，系统会检查当前活跃的事务中的最小LSN（最小未提交的LSN）并记录为组提交的LSN。
3. 所有早于组提交LSN的Redo Log记录都被认为已经提交。
4. 对于还未提交的Redo Log记录，系统会更新其对应数据页上的LSN信息，将其设置为组提交LSN，表示这些操作已经成功提交。

**Binlog组提交过程：**

1. 在Binlog中，每个事务的修改操作会被写入一个或多个事件（Event）中，其中每个事件都有一个唯一的时间戳。

2. 当一个事务提交时，系统会生成一个Commit事件，其中包含了该事务的全部修改操作。
3. Binlog在写入文件时可以选择采用组提交方式，即将多个事务的Commit事件一起写入。
4. 在组提交过程中，系统会根据配置的条件（如时间间隔、日志大小等）来触发Binlog的写入。
5. 一旦触发组提交，系统将会将多个Commit事件一起写入Binlog文件。

**总结起来**，InnoDB的Redo
Log组提交过程中，LSN用于标识事务操作的先后顺序以及判断是否已经提交。对于未提交的操作，其对应的数据页上的LSN会被更新为组提交LSN。而Binlog的组提交过程中，多个事务的Commit事件会一起写入Binlog文件。

**为什么undolog需要记录到redolog中？**

**undolog需要记录到redolog中的原因是为了保证数据库的一致性和持久性。**
在数据库中，当执行事务时，会对数据进行修改和操作。为了保证在事务发生故障或者崩溃时能够恢复到事务执行之前的状态，就需要将undolog记录到redolog中。
Undolog是用来记录数据库中已经发生的事务所做的修改操作，它是一个逻辑日志。当事务需要回滚时，可以通过undolog来撤销对数据的修改操作，使得数据库恢复到事务开始之前的状态。
Redolog是用来记录事务执行过程中所做的修改操作，它是一个物理日志。当事务提交时，redolog会将事务对数据的修改操作记录下来，这样即使在事务提交之后发生故障或崩溃，也可以通过redolog来恢复数据。
通过将undolog记录到redolog中，可以确保数据库在发生故障时可以进行恢复。当系统恢复时，可以根据redolog中的记录重新执行事务，从而保证数据库的一致性和完整性。这种方式可以防止数据丢失和数据不一致的问题，提高了数据库的可靠性和稳定性。

**数据库故障 当前事务执行失败 undolog和redolog的具体恢复过程？**

当数据库发生故障导致当前事务执行失败时，undolog和redolog的具体恢复过程如下： **1. 回滚未完成的事务**
：当事务执行失败时，需要将其回滚到事务开始之前的状态。这可以通过undolog来实现。根据undolog中记录的信息，将事务对数据的修改操作进行撤销，恢复到事务开始之前的状态。
**2. 重做已提交的事务**
：当事务执行成功并提交后，其对数据的修改操作被记录在redolog中。在数据库发生故障后，需要将已提交的事务重新执行，以保证数据的一致性和完整性。这可以通过redolog来实现。根据redolog中的记录，重新执行已提交的事务对数据的修改操作，使得数据库恢复到故障之前的状态。
具体的恢复过程如下： **1. 恢复阶段**： **a. 检查数据库恢复标记**：首先检查数据库的恢复标记，确定是否需要进行恢复操作。 **b.
执行undolog的回滚操作**：对于未提交的事务，根据undolog中的记录，撤销其对数据的修改操作，将数据库恢复到事务开始之前的状态。 **
c. 执行redolog的重做操作：**对于已提交的事务，根据redolog中的记录，重新执行其对数据的修改操作，将数据库恢复到故障之前的状态。
**2. 日志应用阶段**： **a. 应用undolog**：将undolog中的记录应用到数据库中，撤销未提交事务的修改操作。 **b. 应用redolog：**
将redolog中的记录应用到数据库中，重做已提交事务的修改操作。
通过以上的恢复过程，数据库可以在发生故障后进行恢复，确保数据的一致性和完整性。同时，通过undolog和redolog的结合使用，可以提高数据库的可靠性和稳定性。

**磁盘上的undolog文件也可以被清除？**

undolog日志的作用是记录事务中对数据的修改操作，以便在需要回滚事务或提供多版本并发控制（MVCC）时使用。undolog会被刷新到磁盘上的单独的undolog文件中，而不是与redolog一起写入。

在正常事务中，当一个事务执行update或delete操作时，它会生成相应的undolog日志记录，用于撤消该操作。这些undolog日志记录被存储在磁盘上的undolog文件中。当事务提交后，这些undolog记录可以被purge操作删除，因为它们不再需要用于回滚或MVCC。

redolog日志是用来确保数据库的持久性，记录了对数据库所做的物理修改信息。如果在某个时间点下发生数据库宕机，redolog中的信息可以被用来恢复数据库到宕机之前的状态。而undolog文件则用于回滚当前事务。

redolog和undolog具有不同的目的和使用方式，所以它们是分开的。redolog记录的是物理修改信息，而undolog记录的是逻辑操作信息，例如对哪些行进行了修改或删除。这两种日志都是为了保证数据库的完整性和一致性。

**innodb 磁盘上的undo log file 也可以被purge线程清除吗，如何执行的？**

是的，InnoDB磁盘上的undo log file可以被purge线程清除。在InnoDB存储引擎中，事务的撤消操作会生成undo log记录，这些记录保存在磁盘上的undo
log file中。 Purge线程的主要作用是回收不再需要的undo log空间，以便重用。**当事务提交或回滚后，相关的undo
log记录将不再需要，因此purge线程会定期扫描undo log file并删除不再需要的记录。** Purge线程的执行过程如下： 1.
Purge线程首先会扫描InnoDB系统表space id与page number索引，确定哪些页面包含已提交或已回滚的事务。 2.
接下来，Purge线程会检查每个页面上的undo log记录，标记那些已提交的事务并删除相应的undo log记录。 3.
删除过程中，Purge线程会更新相关的数据结构，确保后续的事务操作不会受到影响。
需要注意的是，Purge线程的执行是异步的，并且在高负载情况下可能会有一定的延迟。如果系统长时间运行且没有足够的空间用于重用，那么undo
log file的大小可能会增长，从而占用更多的磁盘空间。为了避免这种情况，可以调整InnoDB的相关参数，例如设置undo
log的大小或调整purge线程的频率，以更好地管理undo log的空间使用。

**innodb undolog是在事务前刷新到磁盘还是事务后？**

**InnoDB的undo log（撤销日志）是在事务进行中刷新到磁盘，而不是事务提交后。**这是因为undo
log在事务执行过程中需要保证持久性和原子性，以支持事务的回滚、并发控制和崩溃恢复。 具体来说，当事务执行修改操作时，InnoDB会将旧值记录到undo
log中。这些undo log记录在内存中进行操作，并在事务进行中持续地刷新到磁盘上的undo log file。这个过程称为"写入undo log"。
通过将undo log持续地刷新到磁盘，可以确保即使系统崩溃或断电，之前已经执行的修改操作也能够被恢复或撤消。此外，持续刷新undo
log还有助于保证事务的原子性，即使在系统故障的情况下，事务的一部分修改操作也不会丢失。 最后，在事务提交后，相关的undo
log记录会被标记为已提交，并在之后的purge线程中进行清理。这样可以保证已提交的事务不再需要undo log，并释放相关的空间，以便其他事务使用。
综上所述，InnoDB的undo log是在事务进行中刷新到磁盘的，以保证事务的持久性、原子性和恢复能力。

**inoodb 一条数据在事务中被提交 undolog日志会被直接删除吗 还是在等待purge判断没有别的事务引用当前版本链的时候，版本链存储的都是什么语句的信息
insert delete update都存储吗？**

当一条数据在事务中被提交时，InnoDB的undo log（撤销日志）不会立即被删除。相反，它会等待purge线程判断当前版本链是否有其他事务引用。
版本链存储了被修改过的数据的历史版本，以支持多版本并发控制（MVCC）机制。每个事务执行插入、删除或更新操作时，相关的旧值都会被记录在undo
log中，并形成一个版本链。这个版本链上的每个版本都包含了被修改的数据以及相应的操作信息。 具体来说，版本链存储了以下类型的语句信息：
1. Insert语句：插入操作会生成一个新的数据版本，并将该版本存储在版本链中。 2.
Delete语句：删除操作不会直接从版本链中删除数据，而是通过标记为已删除，并在后续的purge线程中清理。 3.
Update语句：更新操作会生成一个新的数据版本，并将旧版本和新版本连接在版本链中。
通过保留旧版本的数据和操作信息，InnoDB可以提供一致性读取，即读取在事务开始之前的数据状态，而不受其他并发事务的干扰。同时，在事务回滚或崩溃恢复时，可以使用undo
log来撤消或回滚已提交的事务。 至于undo log的删除，purge线程负责定期扫描版本链并清理不再需要的undo
log记录。当没有其他事务引用版本链中的某个数据版本时，purge线程会判断该版本是否可以被删除，并进行相应的清理操作。
总结起来，InnoDB的undo log在事务提交后不会立即删除，而是等待purge线程判断版本链是否可以被清理。版本链存储了插入、删除和更新操作的相关信息，以支持MVCC机制和事务的回滚与恢复。

### redo log

恢复事务提交修改的页操作，两部分组成：

1、是内存中的重做日志缓冲（redo log buffer）

2、重做日志文件（redo log file）

**重做日志格式**

|               |       |         |               |
|---------------|-------|---------|---------------|
| redo_log_type | space | page_no | redo log body |

**LSN**

LSN是Log Sequence Number的缩写，在InnoDB存储引擎中，表示**事务写入重做日志的字节的总量。**

在每个页的头部，有一个值**FIL_PAGE_LSN**，记录了该页的LSN。在页中，**LSN表示该页最后刷新时LSN的大小。**

用户可以通过命令 **SHOW ENGINE INNODB STATUS** 查看LSN的情况。

```sql
Log sequence number          1083803833
Log buffer assigned up to    1083803833
Log buffer completed up to   1083803833
Log written up to            1083803833
Log flushed up to            1083803833
Added dirty pages up to      1083803833
Pages flushed up to          1083803833
Last checkpoint at           1083803833
```

**Log sequence number**就是当前的redo log(in buffer)中的lsn；
**Log flushed up to**是刷到redo log file on disk中的lsn； 刷新到磁盘文件的的lsn是多少
**Pages flushed up to**是数据页上已经刷到磁盘的LSN dirty页刷新到磁盘的是多少lsn
**Last checkpoint at**是上一次检查点所在位置的LSN。上一次刷新脏页到磁盘的字节量。 上一次检查点的位置

**checkpoint**    表示最后一次检查点的log位置。**它的值表示系统启动时从哪个点去恢复**，redo
log做崩溃恢复时指定的起点。去做崩溃恢复时，终点是最新的一条logfile，起点就是checkpoint，记录的最早脏的点。

**checkpoint的工作机制**

checkpoint有两种方式，**sharp checkpoint**和**fuzzy checkpoint**。

1. **sharp checkpoint：**完全检查点，**数据库正常关闭时，会触发把所有的脏页都写入到磁盘上**，这就是完全检查点，数据库正常运行过程中不会使用sharp
   checkpoint。

2. **fuzzy checkpoint：**模糊检查点，主要有以下四种情况：

- master thread checkpoint：以每秒或者每十秒的速度从缓冲池的脏页列表中刷新一定比例的脏页回磁盘，这个过程是异步的，不会阻塞用户线程。
- flush_lru_list checkpoint：通过参数 **innodb_lru_scan_depth**
  控制LRU列表中可用页的数量，发生了这个checkpoint时，说明脏页写入速度过慢。倘若没有数量的个空闲页，那么InnoDB存储引擎会将LRU列表尾端的页移除，如果这些页中有脏页，那么需要进行Checkpoint。lru链表没有足够的空闲页。
- async/sync flush checkpoint：**当重做日志不可用（即 redo log 写满）时，需要强制将一些页刷新回磁盘此时脏页从脏页列表中获取。
  **。如果不能被覆盖的脏页数量（2-3）达到 75%时，触发异步checkpoint。 不能被覆盖的脏页数量（2-3）达到90%时，同步并且阻塞用户线程，然后根据
  flush 列表最早脏的顺序刷脏页。 当这个事件中的任何一个发生的时候，都会记录到errlog 中，一旦errlog出现这种日志提示，一定需要加大logfile的组数。
  **旧版本中 Async Flush 会阻塞发现问题的用户查询线程，Sync Flush 会阻塞所有查询线程，新版本中在独立的 Page Cleaner Thread
  中执行，不会阻塞**。
- dirty page too much checkpoint：**脏页太多时，也会发生强制写日志**
  ，会阻塞用户线程，由innodb_max_dirty_pages_pct参数（默认90.000000%）控制。 8.0默认90

虽然log block总是在redo log file的最后部分进行写入，有的读者可能以为对redo log file的写入都是顺序的。其实不然，因为redo log
file除了保存log buffer刷新到磁盘的log block，还保存了一些其他信息，这些信息一共占用2kb大小，**即每个redo log
file的前2kb的部分不保存log block的信息**。对于log group 中的第一个redo log file其前2kb的部分保存4个512字节大小的块，其中存放的内容如表所示。

| 名称              | 大小（字节） |
|-----------------|--------|
| log file header | 512    |
| Checkpoin1      | 512    |
| 空               | 512    |
| Checkpoint2     | 512    |

需要特别注意的是，上述信息仅在每个log group的第一个redo log file 中进行存储。log group中的其余redo log
file仅保留这些空间，但不保存上述信息。正因为保存了这些信息，就意味着对redo log file的写入并不是完全顺序的。因为其除了log
block的写入操作，还需要更新前2kb部分的信息，这些信息对于InnoDB存储引擎的回复操作来说非常关键和重要。

例如，页P1的LSN为10000，而数据库启动时，InnoDB检测到写入重做日志中的LSN为13000，并且该事务已经提交，那么数据库需要进行恢复操作，将重做日志应用到P1页中，同样的，对于重做日志中LSN小于P1页的LSN，不需要进行重做，因为P1页中的LSn表示页已经被刷新到该位置。

**控制重做日志刷新到磁盘的策略**

```sql
innodb_flush_log_at_trx_commit
0 提交时不进行写入重做日志操作，仅在master thread每隔一秒钟进行一次fsync重做日志文件的写入
1 默认，表示事务提交时必须调用一次fsync操作
2 表示事务提交时仅写入操作系统的文件缓冲区，有操作系统决定何时写入
```

**binlog和redolog区别**

1. 所处层面不同，binlog时server的，redo时innodb特有的
2. 记录日志内容不同，逻辑binlog记录的是sql语句（原始sql），物理redo记录的是对于每个页的修改（**记录在什么数据页哪个位置修改了多少字节，修改后的数据是什么
   **）
3. binlog只在事务提交完成后进行一次写入，redolog在事务进行中不断地被写入

**为什么会有两份日志呢？**

因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，**binlog 日志只能用于归档
**。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是
redo log 来实现 **crash-safe** 能力。

**redolog**

同样，在 MySQL 里也有这个问题，**如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO
成本、查找成本都很高。**为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。

而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 **Write-Ahead Logging（预写式记录）**
，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。

**现在你就能理解了，WAL 机制主要得益于两个方面：**

1. redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快；
2. 组提交机制，可以大幅度降低磁盘的 IOPS 消耗。

具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，**并更新内存，这个时候更新就算完成了**
。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。

如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。

与此类似，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 *
*4GB** 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。

write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint
是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上
checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**。

**
参考文章：**http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/02%20%20%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%EF%BC%9A%E4%B8%80%E6%9D%A1SQL%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F.md

**组提交机制**

为了提高磁盘fsync的效率，当前数据都提供了group commit的功能，即**一次fsync可以刷新确保多个事务日志被写入文件。**

**这里，我需要先和你介绍日志逻辑序列号（log sequence number，LSN）的概念。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为
length 的 redo log， LSN 的值就会加上 length。**

LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。关于 LSN 和 redo log、checkpoint
的关系，我会在后面的文章中详细展开。

如图 3 所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是
50、120 和 160。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/933fdc052c6339de2aa3bf3f65b188cc.png)

图 3 redo log 组提交

从图中可以看到，

1. trx1 是第一个到达的，会被选为这组的 leader；
2. 等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160；
3. trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘；
4. 这时候 trx2 和 trx3 就可以直接返回了。

所以，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。

在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。

为了让一次 fsync 带的组员更多，MySQL 有一个很有趣的优化：拖时间。在介绍两阶段提交的时候，我曾经给你画了一个图，现在我把它截过来。

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/98b3b4ff7b36d6d72e38029b86870551.png)

图 4 两阶段提交

图中，我把“写 binlog”当成一个动作。但实际上，写 binlog 是分成两步的：

1. 先把 binlog 从 binlog cache 中写到磁盘上的 binlog 文件；
2. 调用 fsync 持久化。

MySQL 为了让组提交的效果更好，把 redo log 做 fsync 的时间拖到了步骤 1 之后。也就是说，上面的图变成了这样：

![img](http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/assets/5ae7d074c34bc5bd55c82781de670c28.png)

图 5 两阶段提交细化

这么一来，binlog 也可以组提交了。在执行图 5 中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog
已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。

不过通常情况下第 3 步执行得会很快，所以 binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此
binlog 的组提交的效果通常不如 redo log 的效果那么好。

如果你想提升 binlog 组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count
来实现。

1. binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync;
2. binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。

这两个条件是或的关系，也就是说只要有一个满足条件就会调用 fsync。

所以，当 binlog_group_commit_sync_delay 设置为 0 的时候，binlog_group_commit_sync_no_delay_count 也无效了。

之前有同学在评论区问到，WAL 机制是减少磁盘写，可是每次提交事务都要写 redo log 和 binlog，这磁盘读写次数也没变少呀？

现在你就能理解了，WAL 机制主要得益于两个方面：

1. redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快；
2. 组提交机制，可以大幅度降低磁盘的 IOPS 消耗。

分析到这里，我们再来回答这个问题：**如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？**

针对这个问题，可以考虑以下三种方法：

1. 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog
   的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。
2. 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志。
3. 将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据。

我不建议你把 innodb_flush_log_at_trx_commit 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL
本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0
其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。

**redo log 的写入机制**

事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。

![image-20230821121501697](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230821121501697.png)

**redolog的三种状态分别是：**

1. 存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分；
2. 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分；
3. 持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。

**为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：**

1. 设置为 0 的时候，表示每次事务提交时都只是把 **redo log留在 redo log buffer 中** ;
2. 设置为 1 的时候，表示每次事务提交时都将 **redo log 直接持久化到磁盘**；
3. 设置为 2 的时候，表示每次事务提交时都只是把 **redo log 写到 page cache**。

InnoDB 有一个后台线程，**每隔 1 秒**，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync
持久化到磁盘。

实际上，除了后台线程每秒一次的轮询操作外，**还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。**

1. **一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。**
   注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。
2. **另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。**假设一个事务 A 执行到一半，已经写了一些
   redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务
   B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。

每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 **redo log 在 commit** 的时候就不需要 fsync 了，只会 write
到文件系统的 page cache 中就够了。**innodb_flush_log_at_trx_commit = 1设置的是在prepare阶段redo log就已经落盘。**

**mysql每执行一条DML语句都先写入 redo log buffer。**

**IOPS**

OPS（Input/Output Operations Per Second）是一个用于计算机存储设备（如硬盘（HDD）、固态硬盘（SSD）或存储区域网络（SAN））性能测试的量测方式，
**可以视为是每秒的读写次数**。

**1、在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象？**

**如果在写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash）**，由于此时 binlog 还没写，redo log 也还没提交，
**所以崩溃恢复的时候，这个事务会回滚**。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。

**2、如果binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理？**

我们先来看一下崩溃恢复时的判断规则。

1. 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；
2. 如果 redo log 里面的事务只有完整的 prepare，**则判断对应的事务 binlog 是否存在并完整**： a. 如果是，则提交事务； b.
   否则，回滚事务。

**3、MySQL 怎么知道 binlog 是完整的?**

回答：一个事务的 binlog 是有完整格式的：

- statement 格式的 binlog，最后会有 **COMMIT**；
- row 格式的 binlog，最后会有一个 XID event。

另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog
日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务
binlog 的完整性的。

**4、redo log 和 binlog 是怎么关联起来的?**

回答：**它们有一个共同的数据字段，叫 XID**。崩溃恢复的时候，会按顺序扫描 redo log：

- 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；
- 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。

**5、处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?**

回答：**其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关**。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候
binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。

所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。

**6、不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？**

如果说**历史原因**的话，那就是 InnoDB 并不是 MySQL 的原生存储引擎。M**ySQL 的原生引擎是 MyISAM，设计之初就有没有支持崩溃恢复。
**

**InnoDB 在作为 MySQL 的插件加入 MySQL 引擎家族之前，就已经是一个提供了崩溃恢复和事务支持的引擎了。**

InnoDB 接入了 MySQL 后，**发现既然 binlog 没有崩溃恢复的能力，那就用 InnoDB 原有的 redo log 好了**。

对于binlog来说如果要恢复的事务处于数据页的最下方，之前已经提交成功的事务是恢复不了的，属于数据页级的丢失。但是，InnoDB
引擎使用的是 WAL 技术，执行事务的时候，写完内存和日志，事务就算完成了。如果之后崩溃，要依赖于日志来恢复数据页。

### bin-log

binlog是Server层实现的二进制日志,他会记录我们的cud操作。Binlog有以下几个特点：

1. Binlog在MySQL的Server层实现（引擎共用）
2. Binlog为逻辑日志,记录的是一条语句的原始逻辑
3. Binlog不限大小,追加写入,不会覆盖以前的日志

**使用bin-log需要先配置my.cnf**

```sql
#查看bin‐log是否开启 
show variables like '%log_bin%'; 
```

**binlog三种格式：**

- **statement**：当binlog=statement时，binlog记录的是SQL本身的语句，语句中可能有函数，比如uuid每次获取都是不一样的，这样同步slave的时候就会出现数据不一致问题
    1. LOAD_FILE()
    2. UUID()
    3. USER()
    4. FOUND_ROWS()
    5. SYSDATE() (除非启动时启用了 --sysdate-is-now 选项)）
- **row：**会记录比如删除delete from table where id < 100,会记录100条被删除每条id的语句，内容占用空间大，当处理随机函数的时候记录的是本次生成的值。
- **mixed：**分析sql，然后决定使用哪种记录方式。

statement格式记录sql原句，**可能会导致主备不一致**
，所以出现了row格式但是row格式也有一个缺点，就是很占空间，比如你delete语句删除1万行记录，statement格式会记录一个sql删除1万行就没了；但是使用row格式会把这1万要删除的记录都写到binlog中，
**这样会导致binlog占用了大量空间同时写binlog也要耗费大量IO**，影响mysql的整体速度。所以MySQL出了个mixed格式，它是前面两种格式的混合。
**意思是MySQL自己会判断这条SQL语句是否会引起主备不一致，是的话就会使用row，否则就用statement格式也就是说上面delete语句加上了limit1
**，MySQL认为会引起主备不一致，它就会使用row格式记录到binlog；如果delete 1万行记录，MySQL认为不会引起主备不一致，它就会使用statement格式记录到binlog。

**binlog的写入时机**

**事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。**一个事务的 binlog
是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。**系统给 binlog cache
分配了一片内存，每个线程一个**，参数 **binlog_cache_size** 用于控制单个线程内 binlog cache
所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。事务提交的时候，执行器把 binlog cache 里的完整事务写入到
binlog 中，并清空 binlog cache

![image-20230820152551606](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230820152551606.png)

可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。

- 图中的 write，指的就是指把日志写入到**文件系统的 page cache**，并没有把数据持久化到磁盘，所以速度比较快。
- 图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。

write 和 fsync 的时机，是由参数 sync_binlog 控制的：

1. sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；
2. sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；
3. sync_binlog=N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。

**binlong常用命令**

```sql
flush logs; #会多一个最新的bin‐log日志 
show master status; #查看最后一个bin‐log日志的相关信息 
reset master; #清空所有的bin‐log日志 
```

redo日志文件：如果事务提交成功，buffer pool里的数据还没来得及写入磁盘，此时系统宕机了，可以用redo日志里的数据恢复buffer
pool里的缓存数据

1.加载缓存数据加载id为1的记录所在的整页数据（16kb）

2.写入更新数据的旧值便于回滚（如果事务提交失败要回滚数据，可以用undo日志里的数据恢复buffer pool里的缓存数据）

3.更新buffer poll 缓存池的数据

4.写redo日志

5.准备提交事务 redo日志写入磁盘

6.准备提交事务 binlog日志吸入磁盘

7.写入commit标记到redo日志文件里，提交事务完成，该标记为了保证事务提交后redo于binlog数据一致

8.随机写入磁盘，以page为单位写入，这步做完磁盘里的数据就是最新的了

**查看bin log**

两种方式**mysqlbinlog命令**和**show binlog events 命令**

mysqlbinlog binlog文件地址

![image-20230820173956428](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230820173956428.png)

```less
show binlog events [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count];
```

这个表示以事件的方式来查看 binlog，这里涉及到几个参数：

log_name：可以指定要查看的 binlog 日志文件名，如果不指定的话，表示查看最早的 binlog 文件。

pos：从哪个 pos 点开始查看，凡是 binlog 记录下来的操作都有一个 pos 点，这个其实就是相当于我们可以指定从哪个操作开始查看日志，如果不指定的话，就是从该
binlog 的开头开始查看。

offset：这是是偏移量，不指定默认就是 0。

row_count：查看多少行记录，不指定就是查看所有。

我们来看一个简单的例子：

```sql
show binlog events in 'javaboy_logbin.000001';
```

![img](https://img-blog.csdnimg.cn/img_convert/65edbcf1100e4442d8a4735bc00ca462.png)

### general log

在默认情况下，MySQL是不会打开general log的，这个log里面会记录MySQL所有的SQL语句，不管是查询语句，还是DML语句，还是DDL语句，还是DCL语句，这些语句统统都会被记录在general
log文件中。就连我们连接和断开MySQL数据库的这些语句。

MySQL会把它收到的所有SQL语句按照接收的顺序依次记录在general
log中。我们需要注意的是，这里接受的SQL语句的顺序，并不等于SQL语句就是按照这个接受的顺序来执行，因为有的时候，一些SQL可能需要等待其他锁被释放后才会被真正的执行，SQL语句的执行顺序是和binlog中的顺序是相匹配的。

假如我们执行一个select查询语句，在binlog中不会记录这样的SQL语句，但是在general log中就会记录这个select查询语句。

General log默认不开启的原因有两个：

日志将会非常大，对磁盘是一个很大的压力。因为所有的操作都会被记录下来。
对MySQL数据的性能有一定的影响。

开启方式两种，配置文件（永久），命令修改（实例临时）

```sql
mysql> show variables like 'general_log'; -- 查看日志是否开启
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| general_log   | OFF   |
+---------------+-------+
1 row in set (0.02 sec)

mysql>
mysql> show variables like 'general_log_file'; -- 看看日志文件保存位置
+------------------+-------------------------+
| Variable_name    | Value                   |
+------------------+-------------------------+
| general_log_file | /var/lib/mysql/test.log |
+------------------+-------------------------+
1 row in set (0.02 sec)

mysql>
mysql> show variables like 'log_output'; -- 看看日志输出类型 table或file
+---------------+------------+
| Variable_name | Value      |
+---------------+------------+
| log_output    | FILE,TABLE |
+---------------+------------+
1 row in set (0.01 sec)

```

# 面试题

## 1. 查看正在进行中的事务

```sql
SELECT * FROM information_schema.INNODB_TRX
```

## 2. 查看正在锁的事务

```sql
#mysql 8.0 版本之前
SELECT * FROM information_schema.INNODB_LOCKS;
#mysql 8.0
select * from performance_schema.data_locks;
```

## 3. 查看等待锁的事务

```sql
#mysql 8.0 版本之前
SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;
#mysql 8.0
select * from performance_schema.data_lock_waits;
```

## 4. 查询是否锁表

```sql
SHOW OPEN TABLES where In_use > 0;
```

## 5. 查看最近死锁的日志

```sql
show engine innodb status
```

## 6.查看表中索引

```sql
show index from department;

show KEYS  from department;

drop index idx_n_a on department;删除索引
```

## 7.**如何获得 MySQL 数据库中最近5分钟更新过的表**？

```sql
SELECT * FROM information_schema.TABLES
WHERE (update_time > DATE_SUB(NOW(), INTERVAL 5 MINUTES)
    AND update_time =< NOW())
```

## 8.查询索引情况

可以通过查询表 mysql.innodb_index_stats 查看每个索引的大致情况

```sql
SELECT 

table_name,index_name,stat_name,

stat_value,stat_description 

FROM innodb_index_stats 

WHERE table_name = 'orders' and index_name = 'PRIMARY';

+----------+------------+-----------+------------+------------------+

|table_name| index_name | stat_name | stat_value |stat_description  |

+----------+-------------------+------------+------------+----------+

| orders | PRIMARY|n_diff_pfx01|5778522     | O_ORDERKEY            |

| orders | PRIMARY|n_leaf_pages|48867 | Number of leaf pages        |

| orders | PRIMARY|size        |49024 | Number of pages in the index|

+--------+--------+------------+------+-----------------------------+

3 rows in set (0.00 sec)
```

## 9.B+树查询未被使用过的索引

```sql
SELECT * FROM sys.schema_unused_indexes WHERE object_schema != 'performance_schema';
而 MySQL 8.0 版本推出了索引不可见（Invisible）功能。在删除废弃索引前，用户可以将索引设置为对优化器不可见，然后观察业务是否有影响。若无，DBA 可以更安心地删除这些索引：

ALTER TABLE t1 

ALTER INDEX idx_name INVISIBLE/VISIBLE;
```

## 10.mysql 查看死锁和解除死锁?

**死锁是指两个或两个以上的事务在执行过程中，因争夺锁资源而造成的一种互相等待的现象。**

解决死锁问题最简单的方法是超时，即当两个事务互相等待时，当一个等待时间超过设置的某一个阈值时，其中一个事务进行回滚，另一个等待的事务就能继续进行。

第二种方式是wait-for
graph（等待图）的方式来进行死锁检测。较为主动的死锁检测方式。InnoDB存储引擎就是采用的这种方式。在每个事务请求锁并发生等待时都会判断是否存在回路，若存在则有死锁，通常来说InnoDB存储引擎选择回滚undo量最小的事务。

查看当前正在进行中的进程

```sql
show processlist
// 也可以使用
SELECT * FROM information_schema.INNODB_TRX;
// 查看正在锁的事务
SELECT * FROM information_schema.innodb_locks;
// 查看等待锁的事务
SELECT * FROM information_schema.innodb_lock_waits;
// 查询是否锁表
show open tables where In_use > 0;
//查看最近死锁的日志
show engine innodb status;
//配置将有关所有死锁的信息打印到mysqld错误日志中。每个死锁的信息，不仅仅是最新的死锁，都记录在mysql错误日志中，完成调试后禁用此选项。
innodb_print_all_deadlocks;
#设置事务等待超时时间
show variables  like  '%innodb_lock_wait_timeout%';
```

![img](https://segmentfault.com/img/remote/1460000038352608)

这两个命令找出来的进程id 是同一个。

杀掉进程对应的进程 id

```command
kill id
```

验证（kill后再看是否还有锁）

```sql
SHOW OPEN TABLES where In_use > 0;
```

```sql
死锁检查，默认开启
show variables like '%innodb_deadlock_detect%';
```

**参考文章：**https://segmentfault.com/a/1190000038352601

## 11.查询长事务？

你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。

```csharp
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
//max_execution_time 是MySQL服务器中的一个用于控制单条语句最大允许执行时间的变量。超过这个时间，MySQL将会终止这条语句的执行，并返回一个错误信息。  
show variables  like  '%max_execution_time%';
```

## 12.MySQL 批量操作，一次插入多少行数据效率最高？

**参考文章：**https://mp.weixin.qq.com/s/RhilSmmfmqwt_mkQUkQiGQ

## 13.Count(1)和Count(*)和Count(列)哪个快？

count(星)计算所有数据中包含null值的行数

count(1)计算所有数据中包含null值的行数

count(列名)计算指定列中不包含null值的行数

**inoodb下count(星)和count(1)一样快，快于count(列名)**

innodb通过遍历最小可用的二级索引来处理语句，如果二级索引不存在，则会扫描聚集索引。

**myisam下count(*)快于或者等于count(1)，快于count(列名)**

myisam的存储了表的总行数，使用count(*)不走统计，直接读取，所以最快，那么当使用count(1)时，假如第一列为not
null，myisam也会直接读取总行数进行优化。

count(列名)因为只统计不为null的，所以要遍历整个表，性能下降。

## 14.MySQL 上亿大表，如何深度优化？

**参考文章：**https://mp.weixin.qq.com/s/TZSyhI1WwrtBX1W86duxuQ

## 15.线上怎么修改列的数据类型的？

方式一:使用mysql5.6 提供的在线修改功能。

ALTER TABLE table_name change old_field_name new_field_name field_type;

那么，在mysql5.5这个版本之前，这是通过临时表拷贝的方式实现的。执行ALTER语句后，会**新建**一个带有新结构的**临时表**
，将原表数据全部拷贝到临时表，然后Rename，完成创建操作。这个方式过程中，原表是可读的，不可写。

## 16.表自增id用完了怎么办

1. 正常自己设置的ID如果满了，在分配的话就会出现主键冲突错误

2. 如果是系统自动分配的6位row_id,满了之后会从0开始，覆盖相匹配数据

3. xid（8字节）,Mysql server层维护，每次执行语句的时候将它赋值给 Query_id，然后给这个变量加 1。如果当前语句是这个事务执行的第一条语句，那么
   MySQL 还会同时把 Query_id 赋值给这个事务的 Xid。**纯内存变量，但是 MySQL 重启之后会重新生成新的 binlog 文件，这就保证了，同一个
   binlog 文件里，Xid 一定是惟一的。从0开始**

4. trx_id（6字节），InnoDB 数据可见性的核心思想是：每一行数据都记录了**更新它的 trx_id**
   ，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的 trx_id 做对比。*
   *当从新从0开始的时候会出现脏读的bug**，而事务 id 从 0 开始计数，就导致了系统在这个时刻之后，所有的查询都会出现脏读的。

5. thread_Id（4字节），满了从0开始，但是，你不会在 show processlist 里看到两个相同的 thread_id。

   这，是因为 MySQL 设计了一个唯一数组的逻辑，给新线程分配 thread_id 的时候，逻辑代码是这样的：

   ```bash
   do {
     new_id= thread_id_counter++;
   } while (!thread_ids.insert_unique(new_id).second);
   ```

   这个代码逻辑简单而且实现优雅，相信你一看就能明白。

**trx_id非连读递增的原因？**

1. update 和 delete 语句除了事务本身，**还涉及到标记删除旧数据，也就是要把数据放到 purge 队列里等待后续物理删除**，这个操作也会把
   max_trx_id+1， 因此在一个事务中至少加 2；
2. InnoDB 的后台操作，**比如表的索引信息统计这类操作**，也是会启动内部事务的，因此你可能看到，trx_id 值并不是按照加 1 递增的。

**
参考文章：**http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2/45%20%20%E8%87%AA%E5%A2%9Eid%E7%94%A8%E5%AE%8C%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F.md

## 17.查找连续id的sql

**连续的数字id-row-number()的值是相同的**

```sql
SELECT *,
             count(*) over ( PARTITION BY t_rank ) as t2_rank
      FROM (SELECT *, id - row_number() over ( ORDER BY id ) as t_rank FROM stadium WHERE people > 99) t

SELECT *, id - row_number() over ( ORDER BY id ) as t_rank FROM stadium WHERE people > 99
2,2071-01-02,109,1
3,2071-01-03,150,1
5,2071-01-05,145,2
6,2071-01-06,1455,2
7,2071-01-07,199,2
8,2071-01-09,188,2

SELECT *,
             count(*) over ( PARTITION BY t_rank ) as t2_rank
      FROM (SELECT *, id - row_number() over ( ORDER BY id ) as t_rank FROM stadium WHERE people > 99) t

2,2071-01-02,109,1,2
3,2071-01-03,150,1,2
5,2071-01-05,145,2,4
6,2071-01-06,1455,2,4
7,2071-01-07,199,2,4
8,2071-01-09,188,2,4

```

## 18.MySQL与redis缓存的同步方案

参考文章：https://blog.csdn.net/androidstarjack/article/details/115191588
方案1：通过MySQL自动同步刷新Redis，MySQL触发器+UDF函数实现

- 在MySQL中对要操作的数据设置触发器Trigger，监听操作
- 客户端（NodeServer）向MySQL中写入数据时，触发器会被触发，触发之后调用MySQL的UDF函数
- UDF函数可以把数据写入到Redis中，从而达到同步的效果
- 这种方案适合于读多写少，并且不存并发写的场景
- 因为MySQL触发器本身就会造成效率的降低，如果一个表经常被操作，这种方案显示是不合适的

方案2：解析MySQL的binlog实现，将数据库中的数据同步到Redis
canal是阿里巴巴旗下的一款开源项目，纯Java开发。基于数据库增量日志解析提供增量数据订阅&消费，目前主要支持了MySQL（也支持mariaDB）

## **19.Mysql分页**

总页数公式：totalRecord是总记录数；pageSize是一页分多少条记录

```java
int totalPageNum = (totalRecord + pageSize - 1) / pageSize;
```

limit分页公式：（curPage - 1）* pageSize，pageSize

```java
limit (curPage - 1) * pageSize, pageSize
```

## 20.有什么命令可以分析加了什么锁？

```sql
我们可以通过 select * from performance_schema.data_locks\G; 这条语句，查看事务执行 SQL 过程中加了什么锁。
```

## **21.数据库中的Schema是什么?**

数据库中schema是**数据库对象集合**，它包含了表，视图等多种对象。schema就像是用户名，当访问数据表时未指明属于哪个schema，系统就会自动的加上缺省的schema。

## 22.mysql数据库与数据库实例的区别？

数据库指的是文件的集合，是按照某种数据模型组织起来的并以二进制存储的数据集合。

数据库实例是应用程序,**是位于用户与操作系统之间的一层数据管理软件**
，用户对数据库进行操作，包括定义表结构，数据查询，数据维护等控制，都是在数据库实例下进行的，可以这样理解，应用程序通过数据库实例才能和数据库打交道。

## 23.MySQL字符集charset和collation的区别和作用?

在MySQL创建数据库时，我们需要指定字符集charset和collation（排序规则），如果不指定，MySQL会自动指定为默认字符集charset和collation（排序规则）。

# 常用命令

## 查看单个SQL最大限制

```sql
show variables like '%max_allowed_packet%';
```

```sql
ROUND() 表示将值x四舍五入为整数，无小数位。

FLOOR(X)表示向下取整，只返回值X的整数部分，小数部分舍弃。 

CEILING(X) 表示向上取整，只返回值X的整数部分，小数部分舍弃。2018年10月9日
```

## 查看SQL执行时间

```sql
show variables like '%profiling%';

set profiling = on;

执行语句

show profiles;查看执行时间
```

## 添加唯一索引

alter table account add unique (appId, accountId)

## 添加字段

alter table 表名 add 字段 类型 其他;

## 修改表字段类型

```sql
alter table book_stock modify stock bigint;
```

## 生成数据脚本

```sql
truncate department;

SET GLOBAL log_bin_trust_function_creators=TRUE; -- 创建函数一定要写这个
DELIMITER $$   -- 写函数之前必须要写，该标志

CREATE FUNCTION mock_data()     -- 创建函数（方法）
RETURNS INT                 -- 返回类型
BEGIN                         -- 函数方法体开始
 DECLARE num INT DEFAULT 1000000;      -- 定义一个变量num为int类型。默认值为100 0000
 DECLARE i INT DEFAULT 0;

 WHILE i < num DO            -- 循环条件
     INSERT INTO department(depno,depname,memo)
     VALUES(i,concat('depname',i),concat('memo',i));
    SET i =  i + 1;    -- i自增
 END WHILE;    -- 循环结束
 RETURN i;
END;

# drop function mock_data;

select mock_data();
```

## OVER 函数 OVER(PARTITION BY… ORDER BY…)

### ROW_NUMBER()

MySQL `ROW_NUMBER()` 函数返回当前行所在的分区内的序号，从 1 开始。

### RANK()

MySQL `RANK()` 函数返回当前行所在的分区内的排名，从 1 开始，但有间隔。

也就是说，相同的值具有相同的排名，但是下一个不同的值的排名采用 [`row_number()`](https://www.sjkjc.com/mysql-ref/row_number/)
编号。比如，如果有 2 个第一名，那么第三位的排名是 `3`。这与 [`dense_rank()`](https://www.sjkjc.com/mysql-ref/dense_rank/)
函数是不同的。

### DENSE_RANK()

MySQL `DENSE_RANK()` 函数返回当前行所在的分区内的排名，从 1 开始，但没有间隔。

也就是说，相同的值具有相同的排名，但是下一个不同的值的排名按顺序增加。比如，如果有 2 个第一名，那么第三位的排名是 `2`
。这与 [`rank()`](https://www.sjkjc.com/mysql-ref/rank/) 函数是不同的。

### NTILE()

MySQL `NTILE()` 函数将当前行所在的分区内的所有行尽可能平均的分成指定数量的区间，并返回当前行所在的区间编号。

每个区间， MySQL 称之为一个排名桶。 `NTILE()` 根据指定排序为每个桶指设定排名。

## 查看磁盘使用量，表大小

```sql
SELECT
table_name,
CONCAT(FORMAT(SUM(data_length) / 1024 / 1024,2),'M') AS dbdata_size,
CONCAT(FORMAT(SUM(index_length) / 1024 / 1024,2),'M') AS dbindex_size,
CONCAT(FORMAT(SUM(data_length + index_length) / 1024 / 1024 / 1024,2),'G') AS table_size,
AVG_ROW_LENGTH,table_rows,update_time
FROM
information_schema.tables
WHERE table_schema = 'badguy' and table_name='t2';
```

## 压缩数据表.ibd

**mydumper并行压缩备份**

```sql
user=root
passwd=xxxx
socket=/datas/mysql/data/3316/mysqld.sock
db=cq_new_cimiss
table_name=arrival_record
backupdir=/datas/dump_$table_name
mkdir -p $backupdir
  nohup echo `date +%T` && mydumper -u $user -p $passwd -S $socket -B $db -c -T $table_name -o $backupdir -t 32 -r 2000000 && echo `date +%T` &
```

**并行压缩备份所花时间（52s）和占用空间（1.2G，实际该表占用磁盘空间为48G，mydumper并行压缩备份压缩比相当高！）**

## **拷贝dump数据到测试节点**

```sql
scp -rp /datas/dump_arrival_record root@10.230.124.19:/datas
```

## **多线程导入数据**

```sql
time myloader -u root -S /datas/mysql/data/3308/mysqld.sock -P 3308 -p root -B test -d /datas/dump_arrival_record -t 32
```

# 数据库设计范式

范式化和反范式话

1.性能提升-冗余，缓存和汇总

2.性能提升-计数器表

3.反范式设计-分库分表中的查询

磁盘读取512字节 有的会是4kb

1.寻道时间 要把磁头移动到某一个确认的硬盘之上 最长

2.旋转时间 寻找匹配的数据

3.传输时间

寻道时间+旋转时间+传输时间

一般是9到10ms左右

前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，**磁盘读取数据靠的是机械运动**，每次读取数据花费的时间可以分为*
*寻道时间、旋转延迟、传输时间**三个部分，**寻道时间指的是磁臂移动到指定磁道所需要的时间**，主流磁盘一般在5ms以下；*
*旋转延迟就是我们经常听说的磁盘转速**
，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；**传输时间指的是从磁盘读出或将数据写入磁盘的时间
**，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 =
9ms左右，听起来还挺不错的，但要知道一台500
-MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。

著作权归@pdai所有
原文链接：https://pdai.tech/md/db/sql-mysql/sql-mysql-index-improve-mt.html

磁盘4k 预读 往往是整数位

成本

I/O成本 1.0

CPU成本 0.2

show table status 表名 表估算值

单表查询的成本

基于成本的优化步骤

1、根据搜索条件，找出所有可能使用的索引

2、计算全表扫描的代价

3、计算使用不同索引执行查询的代价

4、对比各种执行方案的代价，找出成本最低的那一个

预读和局部性原理：

当一个数据被用到时，其附近的数据也通常会马上被使用。

程序运行期间所需要的数据通常比较集中。

in

```
# index dive
show variables like '%dive%';
200
大于两个估算
小于实际计算
```

explain format=json <SQL语句>  /G

![image-20220109155137536](noteImg/image-20220109155137536.png)

索引合并

MySQL在一般情况下执行一个查询时最多只会用到单个二级索引，但存在有特殊情况，在这样特殊情况下也可能在一个查询中使用到多个二级索引，MySQL种这种使用到多个索引来完成一次查询的执行方法称之为：索引合并/index
merge

1. intersection合并 select from order_exp WHERE order_no = 'a' AND expire_time = 'b'

   等值匹配 select from order_exp WHERE order_no>'a' AND insert_time = 'a' AND order_status = 'b' AND expire_time = 'c'

   主键列可以是范围匹配 select from order_exp WHERE id > 100 AND insert_time = 'a'

2. Union合并 select from order_exp WHERE insert_time = 'a' AND order_status = 'b' AND expire_time = 'c' OR (order_no = '
   a' AND expire_time = 'b')

3. Sort—Union合并 select from order_exp WHERE order_no < 'a' OR expire_time > 'z'

## MySQL查询优化规则详解

![image-20220110095426383](noteImg/image-20220110095426383.png)

![image-20220110210140515](noteImg/image-20220110210140515.png)

物化表是指在内存或者磁盘建立一张临时表以查出的列为基础，为临时表建立索引，去掉重复数据后，和外层数据进行关联

## jdbc四大核心对象

Connection

DriverManager

PreparedStatement

ResultSet

![image-20230822174727906](/Users/madongming/IdeaProjects/learn/docs/noteImg/image-20230822174727906.png)

![image-20220115190839410](noteImg/image-20220115190839410.png)

maven-source-plugin maven插件 生成jar包的同时 生成source包

```java
XMLConfigBuilder.java //解析xml文件

XmlMapperBuilder.java //解析mapper文件
```

# 索引

## 聚集索引

指索引项的排序方式和表中数据记录排序方式一致的索引

## 聚簇索引

并不是一种单独的索引类型，而是一种数据存储方式。具体的细节依赖于其实现方式，但InnoDB的聚簇索引实际上在同一个结构中保存了B-Tree索引和数据行。

# 虚拟列 Generated columns

**mysql 5.7 新特性虚拟列**

参考文章：https://www.jianshu.com/p/8447f5aefedd

MySQL的表生成列通常又叫做虚拟列或计算列。这个生成列的值是在列定义时包含了一个计算表达式计算得到的，有两种类型的生成列：

Virtual（虚拟）：这个类型的列会在读取表记录时自动计算此列的结果并返回。
Stored（存储）：这个类型的列会在表中插入一条数据时自动计算对应的值，并插入到这个列中，那么这个列会作为一个常规列存在表中。虚拟生成列有时候比存储生成列更有用，因为它不会占用存储空间。

# 段区页大小

段（segment）
段(Segment)分为**索引段**，**数据段**，**回滚段**
等。其中索引段就是非叶子结点部分，而数据段就是叶子结点部分，回滚段用于数据的回滚和多版本控制。一个段包含256个区(256M大小)。

**一个段包含多少区：**256个区

区（extent）
区是页的集合，一个区包含**64个连续的页**，默认大小为 1MB (64*16K)。

页（page）
页是 InnoDB 管理的最小单位，常见的有 FSP_HDR，INODE, INDEX 等类型。所有页的结构都是一样的，分为文件头(前38字节)
，页数据和文件尾(后8字节)。页数据根据页的类型不同而不一样。

# 查看mysqld的实例是否启动

```java
service mysqld status
```

# mysql的水平分表和垂直分表的区别

### 1、水平分割：

**按表记录拆分**

例：QQ的登录表。假设QQ的用户有100亿，如果只有一张表，每个用户登录的时候[数据库](https://www.2cto.com/database/)
都要从这100亿中查找，会很慢很慢。如果将这一张表分成100份，每张表有1亿条，就小了很多，比如qq0,qq1,qq1...qq99表。

用户登录的时候，可以将用户的id%100，那么会得到0-99的数，查询表的时候，将表名qq跟取模的数连接起来，就构建了表名。比如123456789用户，取模的89，那么就到qq89表查询，查询的时间将会大大缩短。

这就是水平分割。

### 2、垂直分割：

**按字段拆分出新表**

垂直分割指的是：表的记录并不多，但是字段却很长，表占用空间很大，检索表的时候需要执行大量的IO，严重降低了性能。这时需要把大的字段拆分到另一个表，并且该表与原表是一对一的关系。

**参考文章：**https://blog.csdn.net/u012240455/article/details/81952906

# 如何分析mysql-slow.log

**pt-query-digest** 属于 Percona Toolkit 工具集中最常用的一种，号称 MySQL DBA 必备工具之一，其能够分析MySQL数据库的 slow
log 、 general log 、 binary log 文件，同时也可以使用 show processlist 或从tcpdump 抓取的 MySQL 协议数据来进行分析。

默认慢查询log是开启的

```java
show variables like 'slow%';
```

```sql
slow_launch_time,2
slow_query_log,ON
slow_query_log_file,/usr/local/mysql/mysql-slow.log
```

# SQL优化

1. 避免使用select *。
2. 使用union all 代替 union。（会默认比较去重）
3. 小表驱动大表 select in 先处理**子查询** 后处理**外部查询** select exists 先处理**外部查询** 在根据查询结果**跟子查询比较匹配
   **。in适用于用于左边大表，右边小表：exists适用于右边大表，左边小表
4. 批量操作。
5. 多用limit，在删除修改数据时，为了误操作数据，也可以使用limit。
6. in中值太多 多线程分批次查询
7. 增量查询，按id和时间升序，每次保存id和时间，分批次查询
8. 高效的分页 id>指定数 between 分页 要在唯一索引上 子查询使用二级索引
9. 用连接查询代替子查询 （子查询需要额外创建临时表 用完还得删除）
10. join的表不宜过多 选择索引困难 多表数据成几何比较 可以考虑冗余多余字段 不多的情况下
11. join时要注意 inner join可以自动选择小表驱动大表 left join 左表会显示查询的所有表数据，右表会显示符合条件的，如果不符合条件使用null表示
12. 控制索引的数量 需要额外存储空间，维护 阿里巴巴手册规定单表索引和联合索引字段均不超过5个
    高并发系统使用联合索引，删除单个索引，把部分查询功能放到其他数据库中 elasticsearch hbase
13. 选择合理的字段类型 char定长 varchar变长 小的数据类型 bit存布尔值 tinyint 存储枚举值 金额字段用decimal 避免精度丢失
14. 提升group by的效率 去重和分组 having 做耗时操作尽可能缩小数据范围提升sql整体的性能
15. 索引优化

参考视频：https://www.bilibili.com/video/BV1CX4y1Y7rm/?buvid=Z94C2D9EF9DE95124865B3F389C20E6ED774&is_story_h5=false&mid=QxFBuPR633wfOeXFe5jPvA%3D%3D&p=1&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=CCFC39CC-CEBD-4EBF-9EDC-240B3A4630DE&share_source=WEIXIN&share_tag=s_i&timestamp=1692411664&unique_k=ooqXHBp&up_id=1852997180&vd_source=6cb291f20a9b62c2a719b89ee73c2f6f

## 索引失效

**参考文章：**https://www.51cto.com/article/702691.html

1. 联合索引的场景下，查询条件不满足**最左匹配原则。**
2. 使用了select *
3. like查询左边有**%**
4. 索引列使用的**函数计算** left(d,5) == 'abcd' **根据返回值决定是否走索引**
5. 索引列上**有计算** explain select b from t2 where a - 1= 30; **根据返回值决定是否走索引**
6. 字符串类型**没加引号** 类型转换
7. 当查询条件为大于等于、in等范围查询时，根据查询结果占全表数据比例的不同，优化器有可能会放弃索引，进行全表扫描。

# 如果插入数据的值就是sql语句的最大值真的好吗？

>
> 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置`max_allowed_packet`
>
参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用`SELECT *`
> 以及加上`LIMIT`限制的原因之一。

后面通过各种百度，博主觉得最大只是代表传输数据包的最大长度，但性能是不是最佳就要从各个方面来分析了。比如下面列出的插入缓冲，
**以及插入索引时对于缓冲区的剩余空间需求，以及事务占有的内存等**，都会影响批量插入的性能。

1. **首先是插入的时候，要注意缓冲区的大小使用情况**

   如果`buffer pool`余量不足25%，插入失败，返回`DB_LOCK_TABLE_FULL`。这个错误并不是直接报错：`max_allowed_packet`
   不够大之类的，这个错误是因为对于innodb引擎来说，一次插入是涉及到事务和锁的，在插入索引的时候，要判断缓冲区的剩余情况，所以插入并不能仅仅只考虑`max_allowed_packet`
   的问题，也要考虑到缓冲区的大小。

2. **插入缓存**

   另外对于innodb引擎来说，因为存在插入缓存（Insert
   Buffer）这个概念，所以在插入的时候也是要耗费一定的缓冲池内存的。当写密集的情况下，插入缓冲会占用过多的缓冲池内存，默认最大可以占用到1/2的缓冲池内存，当插入缓冲占用太多缓冲池内存的情况下，会影响到其他的操作。

   也就是说，插入缓冲受到缓冲池大小的影响，缓冲池大小为：

   ```sql
   mysql> show variables like 'innodb_buffer_pool_size';
   +-------------------------+-----------+
   | Variable_name           | Value     |
   +-------------------------+-----------+
   | innodb_buffer_pool_size | 134217728 |
   +-------------------------+-----------+
   ```

   换算后的结果为：128M，也就是说，插入缓存最多可以占用64M的缓冲区大小。这个大小要超过咱们设置的sql语句大小，所以可以忽略不计。

   >
   我们都知道，在InnoDB引擎上进行插入操作时，一般需要按照主键顺序进行插入，这样才能获得较高的插入性能。当一张表中存在非聚簇的且不唯一的索引时，在插入时，数据页的存放还是按照主键进行顺序存放，但是对于非聚簇索引叶节点的插入不再是顺序的了，这时就需要离散的访问非聚簇索引页，由于随机读取的存在导致插入操作性能下降。

   InnoDB为此设计了Insert
   Buffer来进行插入优化。对于非聚簇索引的插入或者更新操作，不是每一次都直接插入到索引页中，而是先判断插入的非聚集索引是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个`Insert Buffer`
   中。

   看似数据库这个非聚集的索引已经查到叶节点，而实际没有，这时存放在另外一个位置。然后再以一定的频率和情况进行`Insert Buffer`
   和非聚簇索引页子节点的合并操作。这时通常能够将多个插入合并到一个操作中，这样就大大提高了对于非聚簇索引的插入性能。

3. **使用事务提高效率**

   还有一种说法，使用事务可以提高数据的插入效率，这是因为进行一个`INSERT`操作时，MySQL内部会建立一个事务，在事务内才进行真正插入处理操作。

   事务需要控制大小，事务太大可能会影响执行的效率。MySQL有`innodb_log_buffer_size`
   配置项，超过这个值会把innodb的数据刷到磁盘中，这时，效率会有所下降。所以比较好的做法是，在数据达到这个这个值前进行事务提交。

   查看：`show variables like '%innodb_log_buffer_size%';`

   ```
   +------------------------+----------+
           | Variable_name          | Value    |
           +------------------------+----------+
           | innodb_log_buffer_size | 67108864 |
           +------------------------+----------+
   ```

   大概是：64M

4. **通过配置来提高性能**

   也可以通过增大`innodb_buffer_pool_size` 缓冲区来提升读写性能

5. **索引影响插入性能**

   如果表中存在多个字段索引，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护。这样就降低了数据的插入速度。对于普通的数据表，主键索引是肯定要有的，想要加快性能的话，就是要有序插入，每次插入记录都在索引的最后面，索引的定位效率很高，并且对索引调整较小。如果插入的记录在索引中间，需要B+tree进行分裂合并等处理，会消耗比较多计算资源，并且插入记录的索引定位效率会下降，数据量较大时会有频繁的磁盘操作。

# UNION 操作符

### 描述

MySQL UNION 操作符用于连接两个以上的 SELECT 语句的结果组合到一个结果集合中。多个 SELECT 语句会删除重复的数据。

UNION操作效果和UNION类似会选择出所有重复的值
